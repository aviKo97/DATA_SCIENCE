{
  "subreddit": "computerscience",
  "collection_date": "2025-06-05T14:00:04.821397",
  "total_posts": 200,
  "posts": [
    {
      "id": "1l3ubx1",
      "title": "AI agents is coming for your healthcare. 👀",
      "content": null,
      "author": "WordyBug",
      "created_utc": 1749113561,
      "upvotes": 9,
      "upvote_ratio": 0.74,
      "num_comments": 3,
      "flair": "Discussion",
      "url": "https://i.redd.it/x3lrfl3lo25f1.png",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1l3ubx1/ai_agents_is_coming_for_your_healthcare/"
    },
    {
      "id": "1l3fu4x",
      "title": "What type of research is going on in PL",
      "content": "Exploring potential research paths for grad studies. I have absolutely no PL knowledge/experience, just seems interesting to me.\n\nWhat are some examples of research going on in PL and where’s a good place to get an intro to PL?",
      "author": "GanachePutrid2911",
      "created_utc": 1749068102,
      "upvotes": 21,
      "upvote_ratio": 0.96,
      "num_comments": 9,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1l3fu4x/what_type_of_research_is_going_on_in_pl/"
    },
    {
      "id": "1l37ofe",
      "title": "Computer History",
      "content": "I am in the process of creating a small organisation around teaching people about how to use a computer (starting from zero) which I havent incorperated yet but will either be a charity, a trading company or something inbetween. \n\nI am in the process of writing up a course and felt that it might be appropriate to begin with a short summary of the history of computers, which I begin with Alan Turing to avoid splitting hairs about \"what the first computer was\" and running into ever finer and finer definitions of a computer or suchlike. I aim to end the topic with teaching the very basics of computers - using a mouse and keyboard where I will go on from there.\n\nWhy talk about history when teaching people how to use a computer? \nMy motivation for providing a brief history of computing is that it will subtley introduce some ideas that will be helpful to know when you are learning about how to *use* computers such as \"what is an operating system\". I am a fan of learning the etymology of words because I feel it helps me remember their meaning aswel as being generally interesting to read about (did you know Starbucks comes from a viking name for a river?), im hoping this will have a similar effect to its recipients. \n\nI want to start a discussion on this thread about the history of computers by asking you for anything interesting you know to do with important moments in the development of computers to help my research. I am only 19 so I have never known a world without mobile phones, internet, laser printing and a number of other miracles that I usually take for granted. I would be lying if this wasn't also about a personal curiosity. \nAnything you think is relevant here is welcome for discussion.\n\nThank you :)",
      "author": "External_Resolve_257",
      "created_utc": 1749049006,
      "upvotes": 7,
      "upvote_ratio": 0.82,
      "num_comments": 11,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1l37ofe/computer_history/"
    },
    {
      "id": "1l1xgg5",
      "title": "It's Official: Physics Is Hard (by CS standards)",
      "content": null,
      "author": "VXReload1920",
      "created_utc": 1748908008,
      "upvotes": 28,
      "upvote_ratio": 0.91,
      "num_comments": 11,
      "flair": "Article",
      "url": "https://www.science.org/content/article/its-official-physics-hard",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1l1xgg5/its_official_physics_is_hard_by_cs_standards/"
    },
    {
      "id": "1l1lb1s",
      "title": "Any recommendations on learning and studying System architecture?",
      "content": "Hey y'all, I am Wanting to dip my finger into learning System architecture and wanted to ask for some good resources \n\nThank you",
      "author": "0x426C797A",
      "created_utc": 1748878864,
      "upvotes": 27,
      "upvote_ratio": 0.94,
      "num_comments": 10,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1l1lb1s/any_recommendations_on_learning_and_studying/"
    },
    {
      "id": "1l15rta",
      "title": "How actually did you guys learn reverse engineering?",
      "content": "I am a highschooler, interested in the lowlevel stuffs, in order to learn and explore I tried reverse engineering to see what's inside it and how it's work.\n\nBut it seems kinda overwhelmed for a kid like me, I watched videos on yt and tried to explore dbg/disassembler tools yet still didnt understand what's going on. I didnt find any free course too.\n\nBtw I know basic of computer architecture and how it works in general so I wanna start learning assembly too. Do u have any advice?\n\nI know that I have to know engineering first before step into RE, but I'm open to know how you guys learned.",
      "author": "im-on-meth",
      "created_utc": 1748828222,
      "upvotes": 51,
      "upvote_ratio": 0.87,
      "num_comments": 22,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1l15rta/how_actually_did_you_guys_learn_reverse/"
    },
    {
      "id": "1l0mkz2",
      "title": "What is the digital version of this",
      "content": null,
      "author": "sparrow-head",
      "created_utc": 1748776871,
      "upvotes": 41,
      "upvote_ratio": 0.96,
      "num_comments": 16,
      "flair": null,
      "url": "https://i.redd.it/ms4scz8y894f1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1l0mkz2/what_is_the_digital_version_of_this/"
    },
    {
      "id": "1l0jrkw",
      "title": "Is my paper conference worthy?",
      "content": "Hi all,\n\nI am a PhD student in theoretical computer science and have been working on a side paper for a bit. It deals with a variant of Hierholzer's algorithm for computing a Eulerian cycle in a Eulerian graph that does not require recursion or strict backtracking rules.\n\nTo the best of my knowledge, such a (minor) variant does not exist in the literature, so I would be interested in formalising it and providing a rigorous proof of correctness and complexity. However, since it would be a paper dedicated to a problem that is well studied, I do not know whether it would be conference worthy or deemed redundant.",
      "author": "pastroc",
      "created_utc": 1748765546,
      "upvotes": 18,
      "upvote_ratio": 0.88,
      "num_comments": 6,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1l0jrkw/is_my_paper_conference_worthy/"
    },
    {
      "id": "1l072om",
      "title": "Couldn’t someone reverse a public key’s steps to decrypt?",
      "content": "Hi! I have been trying to understand this for quite some time but it is so confusing…\n\n  \nWhen using a public key to encrypt a message, then why can’t an attacker just use that public key and reverse the exact same steps the public key says to take? \n\nI understand that, for example, mod is often used as if I give you X and W (in the public key), where W = X mod Y, then you multiply your message by W but you still don’t know Y. Which means that whoever knows X would be able to verify that it was truly them (the owner of the private key) due to the infinite number of possibilities but that is of no use in this context?\n\nSo then why can’t I just Divide by W? Or whatever the public key says to do?\n\nSorry if my question is simple but I was really curious and did not understand ChatGPT’s confusing responses!",
      "author": "Cas_07",
      "created_utc": 1748724188,
      "upvotes": 27,
      "upvote_ratio": 0.71,
      "num_comments": 67,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1l072om/couldnt_someone_reverse_a_public_keys_steps_to/"
    },
    {
      "id": "1kz524j",
      "title": "Paper Summary— Jailbreaking Large Language Models with Fewer Than Twenty-Five Targeted Bit-flips",
      "content": null,
      "author": "mohan-aditya05",
      "created_utc": 1748613852,
      "upvotes": 68,
      "upvote_ratio": 0.99,
      "num_comments": 9,
      "flair": "Article",
      "url": "https://pub.towardsai.net/paper-summary-jailbreaking-large-language-models-with-fewer-than-twenty-five-targeted-bit-flips-77ba165950c5?source=friends_link&sk=1c738114dcc21664322f951a96ee7f5b",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kz524j/paper_summary_jailbreaking_large_language_models/"
    },
    {
      "id": "1kzanmy",
      "title": "Guidance for continue learning Computer Architecture",
      "content": "Hello, Im a current final year CS undergrad and throughout my modules I was exposed to some ideas of Computer systems, OS, and Computer architecture and Compiler theory. I know the basics of many things but I would like to learn in depth, especially in CA. I was exposed the basics of pipelining, parallelism, multithreading, virutal memory and caches etc. The H&P book was refered in a module so naturally I would finish reading that. Apart from that where can I take the next steps towards to, with my current high level exposure to the ideas? \n\nIve heard about the;\n\n nand2tetris, Computer Systems: A Programmer's Perspective, Tenebaum's \"Modern Operating Systems\", \"Code: The Hidden Language of Computer Hardware and Software\", Ben Eater\"s Build an 8-bit computer from scratch etc.\n\nIs there any resources here that would repeat what I already know? Or is there any recommended resource that I can take to continue? Or any order? I had a very unstructured learning of the theories and confused about the best place to continue.  \n\nWould really appreciate any advice. Thanks in advance",
      "author": "ECHOSTIK",
      "created_utc": 1748627274,
      "upvotes": 12,
      "upvote_ratio": 1.0,
      "num_comments": 4,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kzanmy/guidance_for_continue_learning_computer/"
    },
    {
      "id": "1kye7k5",
      "title": "How much CS do I need to be familiar with to learn theoretical computer science?",
      "content": "I'm really interested in mathematical logic, and its often involved in theoretical computer science. I know basically nothing about cs, but the little glimpses I have into theoretical cs make it seem really interesting. \nI don't want to study it professionally or academically, just for fun and maybe to see how it relates to math. I'm not worrying about applying anything personally or doing projects, I just want to learn about it.\nI don't want to try jumping in without the right background knowledge and either be completely lost or misinterpret it. \nI would just be learning introductory stuff, not any specific subfield \nWhat basic computer science is necessary to kind of get the gist? Do I need to be familiar with a certain programming language? I don't much about computing at all, so I'm kind of going in blind.",
      "author": "grahamio",
      "created_utc": 1748533716,
      "upvotes": 83,
      "upvote_ratio": 0.93,
      "num_comments": 43,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kye7k5/how_much_cs_do_i_need_to_be_familiar_with_to/"
    },
    {
      "id": "1kyh2oo",
      "title": "Will quantum computers ever be available to everyday consumers, or will the always be exclusively used by companies, governments, and researchers?",
      "content": "I understand that they probably won't replace standard computers, but will there be some point in the future where computers with quantum technology will be offered to consumers as options alongside regular machines?",
      "author": "Pineapple_Gamer123",
      "created_utc": 1748540499,
      "upvotes": 12,
      "upvote_ratio": 0.77,
      "num_comments": 48,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kyh2oo/will_quantum_computers_ever_be_available_to/"
    },
    {
      "id": "1kxeexs",
      "title": "New algorithm beats Dijkstra's time for shortest paths in directed graphs",
      "content": null,
      "author": "RogueCookie9586",
      "created_utc": 1748430896,
      "upvotes": 980,
      "upvote_ratio": 0.99,
      "num_comments": 56,
      "flair": null,
      "url": "https://arxiv.org/abs/2504.17033",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kxeexs/new_algorithm_beats_dijkstras_time_for_shortest/"
    },
    {
      "id": "1kwyc4t",
      "title": "Does memoizing a function make it truly \"idempotent\"?",
      "content": "If you cache the result of a function, or say, for instance, check to see if its already been run, and skipping running it a second time make a function truly idempotent?",
      "author": "dashdanw",
      "created_utc": 1748378749,
      "upvotes": 20,
      "upvote_ratio": 0.78,
      "num_comments": 39,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kwyc4t/does_memoizing_a_function_make_it_truly_idempotent/"
    },
    {
      "id": "1kwg74n",
      "title": "What do you think is next gamechanging technology?",
      "content": "Hi, Im just wondering what are your views on prospets of next gamechanging technology? What is lets say docker of 2012/15 of today? The only thing I can think of are softwares for automation in postquantum migration cause it will be required even if quantum computing wont mature.",
      "author": "arktozc",
      "created_utc": 1748326599,
      "upvotes": 23,
      "upvote_ratio": 0.79,
      "num_comments": 25,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kwg74n/what_do_you_think_is_next_gamechanging_technology/"
    },
    {
      "id": "1kvsyex",
      "title": "Resource on low level math optimisation",
      "content": "Hello people. Im currently making a FEM matrix assembler. I want to have it work as efficiently as possible. Im currently programming it in python+numba but i might switch to Rust. \nI want to learn more about how to write code in a way that the compiler can optimise it as well as possible. I dont know if the programming language makes night and day differences but i feel like in general there should be information on heuristics that will guide me in writing my code so that it runs as fast as possible. I do understand that some compilers are more efficient at finding these optimisations than others. The type of stuff I’m referring to could be for example (pseudo code)\n\nf(0,0) = a*b + c*d\nf(1,0) = a*b - c*d\n\nvs \n\nq1 = a*b\nq2 = c*d\nf(0,0) = q1+q2\nf(1,0) = q1-q2\n\nDoes anyone know of videos/books/webpages to consult?\n",
      "author": "HuygensFresnel",
      "created_utc": 1748262012,
      "upvotes": 15,
      "upvote_ratio": 0.89,
      "num_comments": 9,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kvsyex/resource_on_low_level_math_optimisation/"
    },
    {
      "id": "1kvfo27",
      "title": "What exactly differentiates data structures?",
      "content": "I've been thinking back on the DSA fundamentals recently while designing a new system, and i realised i don't really know where the line is drawn between different data structures.\n\nIt seems to be largely theoretical, as stacks, arrays, and queues are all udually implemented as arrays anyway, but what exactly is the discriminating quality of these if they can all be implemented at the same time?\n\nIs it just the unique combination of a structure's operational time complexity (insert, remove, retrieve, etc) that gives it its own 'category', or something more?",
      "author": "KJBuilds",
      "created_utc": 1748214444,
      "upvotes": 30,
      "upvote_ratio": 0.78,
      "num_comments": 32,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kvfo27/what_exactly_differentiates_data_structures/"
    },
    {
      "id": "1kv6nds",
      "title": "Alan Turing papers saved from shredder to be sold in Lichfield (UK) June 17",
      "content": null,
      "author": "Maui96793",
      "created_utc": 1748190534,
      "upvotes": 45,
      "upvote_ratio": 0.96,
      "num_comments": 1,
      "flair": null,
      "url": "https://www.bbc.com/news/articles/cwyvq7n5979o",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kv6nds/alan_turing_papers_saved_from_shredder_to_be_sold/"
    },
    {
      "id": "1ku38kw",
      "title": "One CS class, and now I'm addicted",
      "content": "I have taken a single college course on C++, and this is what it has brought me to. I saw a post about the birthday problem (if you don't know, it's a quick Google), and thought, \"I bet I can write a program to test this with a pretty large sample size\". Now here I am 1.5 hours later, with a program that tests the birthday problem with a range of group sizes from 1 to 100. It turns out it's true, at 23 people, there is a 50% chance of a shared birthday. ",
      "author": "ChickenFeline0",
      "created_utc": 1748061048,
      "upvotes": 449,
      "upvote_ratio": 0.93,
      "num_comments": 58,
      "flair": "General",
      "url": "https://www.reddit.com/gallery/1ku38kw",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1ku38kw/one_cs_class_and_now_im_addicted/"
    },
    {
      "id": "1kuhxok",
      "title": "Anyone have tips for how I should study compilers?",
      "content": "How can I go about learning compilers quickly and efficiently. Anyone have good links for -  but not limited to - learning about virtual machines, parsing machines, and abstract syntax trees? ",
      "author": "Dry_Growth_1605",
      "created_utc": 1748110091,
      "upvotes": 5,
      "upvote_ratio": 0.73,
      "num_comments": 8,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kuhxok/anyone_have_tips_for_how_i_should_study_compilers/"
    },
    {
      "id": "1kubw3s",
      "title": "What’s your process when you can’t trace how a system reaches its results?",
      "content": "I regularly find myself in situations where I'm using a tool, library, or model that returns answers or outputs, but I can't see the process it follows to get there. If something doesn't seem quite right, strange, or surprising, it can be difficult to figure out what is going on behind the scenes and how to get to the bottom of the issue. If you have experienced a similar situation when you have had to work with something you don't feel comfortable fully inspecting what techniques do you take to either assess, understand, or simply build confidence in what it is doing? ",
      "author": "nvntexe",
      "created_utc": 1748094076,
      "upvotes": 6,
      "upvote_ratio": 0.75,
      "num_comments": 9,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kubw3s/whats_your_process_when_you_cant_trace_how_a/"
    },
    {
      "id": "1ktfi35",
      "title": "C or C++ or some other lang",
      "content": "I was thinking of learning a new lang, i want to pursue computer science eng, which is the best to learn for future\n\ni know some basics of python and C,\n\nI can allocate around an hour or two daily for atleast a year\n\ni definitely want to go into game development or software development or some thing related to micro computers or microprocessors.",
      "author": "katozukazi",
      "created_utc": 1747993570,
      "upvotes": 14,
      "upvote_ratio": 0.71,
      "num_comments": 35,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ktfi35/c_or_c_or_some_other_lang/"
    },
    {
      "id": "1ksd8sc",
      "title": "Why Are Recursive Functions Used?",
      "content": "Why are recursive functions sometimes used? If you want to do something multiple times, wouldn't a \"while\" loop in C and it's equivalent in other languages be enough? I am not talking about nested data structures like linked lists where each node has data and a pointed to another node, but a function which calls itself.",
      "author": "ShadowGuyinRealLife",
      "created_utc": 1747872639,
      "upvotes": 106,
      "upvote_ratio": 0.79,
      "num_comments": 150,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ksd8sc/why_are_recursive_functions_used/"
    },
    {
      "id": "1ks16n4",
      "title": "Best cs book you ever read?",
      "content": "Hi all, what's the best computer science book you've ever read that truly helped you in your career or studies? I'd love to hear which book made a real difference for you and why.",
      "author": "SubstantialCause00",
      "created_utc": 1747842458,
      "upvotes": 126,
      "upvote_ratio": 0.99,
      "num_comments": 50,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ks16n4/best_cs_book_you_ever_read/"
    },
    {
      "id": "1krv0xy",
      "title": "why is f(x) = |x^0.5| a function and why is f(x) = x^0.5 not a function?",
      "content": "https://preview.redd.it/0166xw25a42f1.png?width=945&format=png&auto=webp&s=7efc7c8b6a462bede99c4cfa939b99e283710a90\n\n",
      "author": "Different-Project940",
      "created_utc": 1747825406,
      "upvotes": 5,
      "upvote_ratio": 0.54,
      "num_comments": 114,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1krv0xy/why_is_fx_x05_a_function_and_why_is_fx_x05_not_a/"
    },
    {
      "id": "1kr6pyj",
      "title": "Computing pioneer Alan Turing’s early work on “Can machines think?” published in a 1950 scholarly journal sold at the Swann Auction sale of April 22 for $10,000 or double the pre sale high estimate. Reported by RareBookHub.com",
      "content": "The catalog described the item as: Turing, Alan (1912-1954), Computing, Machinery, and Intelligence, published in Mind: a Quarterly Review of Psychology and Philosophy. Edinburgh: Thomas Nelson & Sons, Ltd., 1950, Vol. LIX, No. 236, October 1950.\n\n\n\nFirst edition of Turing's essays posing the question, \"Can machines think?\"; limp octavo-format, the complete journal in publisher's printed paper wrappers, with Turing's piece the first to appear in the journal, occupying pages 433-460.\n\n\n\nThe catalog comments: “With his interest in machine learning, Turing describes a three-person party game in the present essay that he calls the imitation game. Also known as the Turing test, its aim was to gauge a computer's capacity to interact intelligently through questions posed by a human. Passing the Turing test is achieved when the human questioner is convinced that they are conversing by text with another human. In 2025, many iterations of AI pass this test.” ",
      "author": "Hammer_Price",
      "created_utc": 1747752448,
      "upvotes": 40,
      "upvote_ratio": 0.94,
      "num_comments": 10,
      "flair": null,
      "url": "https://i.redd.it/fhq20fk19y1f1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kr6pyj/computing_pioneer_alan_turings_early_work_on_can/"
    },
    {
      "id": "1kr4rhv",
      "title": "Anyone here building research-based HFT/LFT projects? Let’s talk C++, models, frameworks",
      "content": "I’ve been learning and experimenting with both C++ and Python — C++ mainly for understanding how low-latency systems are actually structured, like:\n\nMulti-threaded order matching engines\n\nEvent-driven trade simulators\n\nLow-latency queue processing using lock-free data structures\n\nCustom backtest engines using C++ STL + maybe Boost/Asio for async simulation\n\nTrying to design modular architecture for strategy plug-ins\n\n\nI’m using Python for faster prototyping of:\n\nSignal generation (momentum, mean-reversion, basic stat arb models)\n\nFeature engineering for alpha\n\nPlotting and analytics (matplotlib, seaborn)\n\nBacktesting on tick or bar data (using backtesting.py, zipline, etc.)\n\n\nRecently started reading papers from arXiv and SSRN about market microstructure, limit order book modeling, and execution strategies like TWAP/VWAP and iceberg orders. It’s mind-blowing how much quant theory and system design blend in this space.\n\nSo I wanted to ask:\n\nAnyone else working on HFT/LFT projects with a research-ish angle?\n\nAny open-source or collaborative frameworks/projects you’re building or know of?\n\nHow do you guys structure your backtesting frameworks or data pipelines? Especially if you're also trying to use C++ for speed?\n\nHow are you generating or accessing tick-level or millisecond-resolution data for testing?\n\n\nI know I’m just starting out, but I’m serious about learning and contributing neven if it’s just writing test modules, documentation, or experimenting with new ideas. If any of you are building something in this domain, even if it’s half-baked, I’d love to hear about it.\n\nLet’s connect and maybe even collab on something that blends code + math + markets.\nPeace.",
      "author": "dronzabeast99",
      "created_utc": 1747747388,
      "upvotes": 7,
      "upvote_ratio": 1.0,
      "num_comments": 6,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kr4rhv/anyone_here_building_researchbased_hftlft/"
    },
    {
      "id": "1kqrqjc",
      "title": "Why are people worried about quantum computing cracking codes so fast if the application of attempting all the possible combinations is still limited by traditional computing speeds of the devices being cracked?",
      "content": null,
      "author": "Intelligent-Row2687",
      "created_utc": 1747701006,
      "upvotes": 26,
      "upvote_ratio": 0.73,
      "num_comments": 40,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kqrqjc/why_are_people_worried_about_quantum_computing/"
    },
    {
      "id": "1kpu092",
      "title": "A computer scientist's perspective on vibe coding:",
      "content": null,
      "author": "eternviking",
      "created_utc": 1747601148,
      "upvotes": 3472,
      "upvote_ratio": 0.98,
      "num_comments": 246,
      "flair": null,
      "url": "https://i.redd.it/oi8nz7obrl1f1.png",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kpu092/a_computer_scientists_perspective_on_vibe_coding/"
    },
    {
      "id": "1kqih1q",
      "title": "Assembly IDE in the Web: Learn MIPS, RISC-V, M68K, X86 assembly",
      "content": "Hello everyone!  \nDuring my first CS year i struggled with systems programming (M68K and MIPS assembly) because the simulators/editors that were suggested to us were outdated and lacked many useful features, especially when getting into recursion.\n\nThat's why i made [https://asm-editor.specy.app/](https://asm-editor.specy.app/), a Web IDE/simulator for MIPS, RISC-V, M68K, X86 (and more in the future) Assembly languages.\n\nIt's open source at [https://github.com/Specy/asm-editor](https://github.com/Specy/asm-editor), Here is a [recursive fibonacci function in MIPS](https://shorturl.at/CygXX) to show the different features of the IDE. \n\nhttps://preview.redd.it/10olx6fm1s1f1.png?width=1918&format=png&auto=webp&s=bb14278cc5539752a175dcb2583ee430bcead633\n\nSome of the most useful features are:\n\n* instruction undo and step \n* breakpoints\n* function stack tracing and stack frame view. \n* history viewer (shows the side effects of each instruction)\n* I/O and memory viewer (both number and text)\n* number conversions for registers and memory \n* testcases (useful for professors making exercises)\n* auto completion and inline errors, etc...\n\nThere is also a feature to embed the editor inside other websites, so if you are a professor making courses, or want to use the editor inside your own website, you can!\n\nLast thing, i just finished implementing a feature that allows interactive courses to be created. If you are experienced in assembly languges and want to help other students, come over on the [github repo](https://github.com/Specy/asm-editor/issues/27) to contribute!",
      "author": "specy_dev",
      "created_utc": 1747677665,
      "upvotes": 7,
      "upvote_ratio": 0.82,
      "num_comments": 3,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kqih1q/assembly_ide_in_the_web_learn_mips_riscv_m68k_x86/"
    },
    {
      "id": "1kq8lge",
      "title": "When is a deck of cards \"truly shuffled\"?",
      "content": "Hey! I wrote this article recently about mixing times for markov chains using deck shuffling as the main example. It has some visualizations and explains the concept of \"coupling\" in what-I-hope a more intuitive way than typical textbooks.\n\nLooking for any feedback to improve my writing style + visualization aspects in these sort of semi-academic settings.",
      "author": "Due_Raspberry_6269",
      "created_utc": 1747651593,
      "upvotes": 5,
      "upvote_ratio": 1.0,
      "num_comments": 2,
      "flair": "Article",
      "url": "https://www.sidhantbansal.com/2025/When-is-a-deck-truly-shuffled/",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kq8lge/when_is_a_deck_of_cards_truly_shuffled/"
    },
    {
      "id": "1kq0276",
      "title": "Built simple http server in c",
      "content": "I've built a simple HTTP server in C \nIt can handle multiple requests, serve basic HTML and image files, and log what's happening.\nI learned a lot about how servers actually work behind the scenes.\n\nGithub repo : https://github.com/sandeepsandy62/Httpserver",
      "author": "sandeepgogarla27",
      "created_utc": 1747618806,
      "upvotes": 13,
      "upvote_ratio": 0.88,
      "num_comments": 0,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kq0276/built_simple_http_server_in_c/"
    },
    {
      "id": "1kp5o55",
      "title": "Is it worth pursuing an alternative to SIMT using CPU-side DAG scheduling to reduce branch divergence?",
      "content": "Hi everyone,\nThis is my first time posting here, and I’m genuinely excited to join the community.\n\nI’m an 18-year-old self-taught enthusiast deeply interested in computer architecture and execution models. Lately, I’ve been experimenting with an alternative GPU-inspired compute model — but instead of following traditional SIMT, I’m exploring a DAG-based task scheduling system that attempts to handle branch divergence more gracefully.\n\nThe core idea is this: instead of locking threads into a fixed warp-wide control flow, I decompose complex compute kernels (like ray intersection logic) into smaller tasks with explicit dependencies. These tasks are then scheduled via a DAG, somewhat similar to how out-of-order CPUs resolve instruction dependencies, but on a thread/task level. There's no speculative execution or branch prediction; the model simply avoids divergence by isolating independent paths early on.\n\nAll of this is currently simulated entirely on the CPU, so there's no true parallel hardware involved. But I've tried to keep the execution model consistent with GPU-like constraints — warp-style groupings, shared scheduling, etc. In early tests (on raytracing workloads), this approach actually outperformed my baseline SIMT-style simulation. I even did a bit of statistical analysis, and the p-value was somewhere around 0.0005 or 0.005 — so it wasn't just noise.\n\nAlso, one interesting result from my experiments:\nWhen I lock the thread count using constexpr at compile time, I get around 73–75% faster execution with my DAG-based compute model compared to my SIMT-style baseline.\n\nHowever, when I retrieve the thread count dynamically using argc/argv (so the thread count is decided at runtime), the performance boost drops to just 3–5%.\n\nI assume this is because the compiler can aggressively optimize when the thread count is known at compile time, possibly unrolling or pre-distributing tasks more efficiently. But when it’s dynamic, the runtime cost of thread setup and task distribution increases, and optimizations are limited.\n\nThat said, the complexity is growing. Task decomposition, dependency tracking, and memory overhead are becoming a serious concern. So, I’m at a crossroads:\nShould I continue pursuing this as a legitimate alternative model, or is it just an overengineered idea that fundamentally conflicts with what makes SIMT efficient in practice?\n\nSo as title goes, should I go behind of this idea? I’d love to hear your thoughts, even if critical. I’m very open to feedback, suggestions, or just discussion in general. Thanks for reading!",
      "author": "IsimsizKahraman81",
      "created_utc": 1747522699,
      "upvotes": 10,
      "upvote_ratio": 0.86,
      "num_comments": 9,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kp5o55/is_it_worth_pursuing_an_alternative_to_simt_using/"
    },
    {
      "id": "1knipc1",
      "title": "Stack Overflow is dead.",
      "content": "This graph shows the volume of questions asked on Stack Overflow. The number is now almost equal to when the site was initially launched. So, it is safe to say that Stack Overflow is virtually dead.",
      "author": "eternviking",
      "created_utc": 1747341454,
      "upvotes": 9547,
      "upvote_ratio": 0.98,
      "num_comments": 975,
      "flair": null,
      "url": "https://i.redd.it/d6gragjpa01f1.png",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1knipc1/stack_overflow_is_dead/"
    },
    {
      "id": "1koinxc",
      "title": "Book recommendations?",
      "content": "Hello everyone! I was hoping for some help with book recommendations about chips. I’m currently reading The Thinking Machine by Stephen Witt, and planning to read Chip Wars along with a few other books about the history and impact of computer chips. I’m super interested in this topic and looking for a more technical book to explain the ins and outs of computer hardware/architecture rather than a more journalistic approach on the topic, which is what I’ve been reading.  \n\nThank you!!",
      "author": "duckofthewest",
      "created_utc": 1747449730,
      "upvotes": 5,
      "upvote_ratio": 0.78,
      "num_comments": 3,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1koinxc/book_recommendations/"
    },
    {
      "id": "1knwhiu",
      "title": "Machine learning used to be cool, no?",
      "content": "Remember deepdream, aidungeon 1, those reinforcement learning and evolutionary algorithm showcases on youtube? Was it all leading to this nightmare? Is actually fun machine learning research still happening, beyond applications of shoehorning text prediction and on-demand audiovisual slop into all aspects of human activity? Is it too late to put the virtual idiots we've created back into their respective genie bottles?",
      "author": "Own_Schedule_5536",
      "created_utc": 1747387269,
      "upvotes": 97,
      "upvote_ratio": 0.94,
      "num_comments": 33,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1knwhiu/machine_learning_used_to_be_cool_no/"
    },
    {
      "id": "1kn159w",
      "title": "Most underground and unknown stuff",
      "content": "Which kind of knowledge you think is really underground and interesting, but usually nobody looks up?",
      "author": "lowiemelatonin",
      "created_utc": 1747289447,
      "upvotes": 35,
      "upvote_ratio": 0.95,
      "num_comments": 22,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kn159w/most_underground_and_unknown_stuff/"
    },
    {
      "id": "1kmiqoe",
      "title": "Is this an accurate diagram of a CPU block?",
      "content": "I am doing a university module of computer systems and security. It is a Time Constraint Assessment so I have little idea of what the questions will be, but I am of the assumption that it will be things like \"explain the function of X\". In one of the online supplementary lessons there is a brief description of a CPU and a crude diagram with modals to see more about each component, but looking at diagrams from other sources I am getting conflicting messages.\n\n  \nFrom what I've gather from the various diagrams, this is what I came to. I haven't added any data bus and control bus arrows yet, but for the most part they're just 2 way arrows between each of the components which I don't really get because I was under the impression the Fetch-Decode-Execute was a cycle and cycles usually go round linearly.\n\n  \nWould you say this is an accurate representation of a CPU block? If not, what specifically could I add/change/remove to improve it?",
      "author": "Its_An_Outraage",
      "created_utc": 1747238131,
      "upvotes": 83,
      "upvote_ratio": 0.9,
      "num_comments": 26,
      "flair": "Advice",
      "url": "https://i.redd.it/rpb7jpp1qr0f1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kmiqoe/is_this_an_accurate_diagram_of_a_cpu_block/"
    },
    {
      "id": "1kmpnbd",
      "title": "Designing an 8-bit CPU: How to load constants?",
      "content": "I have been following Sebastian Lague's videos on YouTube and have started to make my own CPU in his Digital Logic Sim. Currently it is single cycle and I have registers A and B, a program counter, a basic ALU and ROM for the program.\n\nMy goal is to run a program that outputs the Fibonacci sequence. I have a very basic control unit which has output bits for:\n\n* Write to A\n* Write to B\n* Output A\n* Output B\n* Output ALU\n\nWith this I have made an ADD instruction which adds A and B and writes the output to A. \n\nI now need an instruction to load a constant into either A/B. I've looked online but am quite confused how to implement this. I've seen examples which have the immediate constant, e.g.: XXXXAAAA, where X is the opcode and A is the constant (ideally I want to learn how to load 8 bit numbers, so this won't work for me).\n\nI've seen other examples where it uses microcode and 2 bytes, e.g.: the first byte is the instruction to load a constant, and the second is the actual constant (which would allow for 8 bits).\n\nWhat would be the best way to implement the microcode? Would this be possible for a single cycle CPU? Do I need an instruction register? I also don't want the CPU to execute the data, so how would I correctly increment the program counter? Just increment it twice?",
      "author": "zinc__88",
      "created_utc": 1747254581,
      "upvotes": 6,
      "upvote_ratio": 0.88,
      "num_comments": 9,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kmpnbd/designing_an_8bit_cpu_how_to_load_constants/"
    },
    {
      "id": "1kmalt7",
      "title": "How screwed am I if I don’t know discrete math",
      "content": "I did a discrete math course and it was an awful time. It was online and the professor just read from the textbook. Asking question and taking note did not help.I did not drop it because it was my first time as a student in higher level education so I was scared but now I regret it. In the end they rounded up grades. It has been a while and I have forgoten what little I had learned. I know that it is used in artificial intelligent classes and others. I have the option to do the course again in different environment. But I want to know what would happen if I take these classes with no information in discrete math.",
      "author": "Otherwise_Plane_4048",
      "created_utc": 1747213668,
      "upvotes": 35,
      "upvote_ratio": 0.81,
      "num_comments": 32,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kmalt7/how_screwed_am_i_if_i_dont_know_discrete_math/"
    },
    {
      "id": "1kl22sy",
      "title": "Basic question about parallel repetition in IP protocol",
      "content": "The book Sanjeev Arora and Barak defines class IP (\\[interactive protocol\\]\\[1\\]) by making the verifier have private coins. Before proceeding to public coin proofs and showing they are the \"same,\" the book mentions the following:\n\n\n\n\\> The probabilities of correctly classifying an input can be made arbitrarily close to 1 by using\n\n the same boosting technique we used for BPP: to replace $2/3$ by $1−e\\^{−m}$,\n\n sequentially repeat the protocol m times and take the majority answer. In fact, using a more\n\n complicated proof, it can be shown that we can decrease the probability without increasing the\n\n number of rounds using parallel repetition (i.e., the prover and verifier will run $m$ executions\n\n of the protocol in parallel). \n\n\n\nWhy does the naive idea of simply having the verfier and prover exchange an array of polynomial many messages  (different  copies) in each round not work? This doesn't increase the rounds. Assuming that for each copy, the verifier uses independent random coins.\n\n\n\n\n\n\n\n  \\[1\\]: [https://en.wikipedia.org/wiki/Interactive\\_proof\\_system](https://en.wikipedia.org/wiki/Interactive_proof_system)",
      "author": "Only_Exit3948",
      "created_utc": 1747079364,
      "upvotes": 12,
      "upvote_ratio": 0.93,
      "num_comments": 0,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kl22sy/basic_question_about_parallel_repetition_in_ip/"
    },
    {
      "id": "1kk8102",
      "title": "What books would you recommend as an introduction to computer science?",
      "content": "I'm not looking for a book on coding languages, rather I'm looking to focus on the fundamentals. I've been recommended, Code: the hidden language of computer hardware and software 2nd edition. What do you all think?",
      "author": "MudCandid8006",
      "created_utc": 1746988850,
      "upvotes": 60,
      "upvote_ratio": 0.95,
      "num_comments": 38,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kk8102/what_books_would_you_recommend_as_an_introduction/"
    },
    {
      "id": "1kjffzv",
      "title": "Flip Flops and Stochastic Processes",
      "content": "\n\nFlip flops are components within computer architecture which can store and manipulate data. The output of the flip flop is dependent on past events. So, could you model flip flops as a stochastic process like a Markov chain?  \n",
      "author": "preetluvsu",
      "created_utc": 1746897793,
      "upvotes": 13,
      "upvote_ratio": 0.94,
      "num_comments": 1,
      "flair": null,
      "url": "https://i.redd.it/o02kayd0ozze1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kjffzv/flip_flops_and_stochastic_processes/"
    },
    {
      "id": "1kikar2",
      "title": "Hashing isn’t just for lookups: How randomness helps estimate the size of huge sets",
      "content": "Link to blog: [https://www.sidhantbansal.com/2025/Hashing-when-you-want-chaos/](https://www.sidhantbansal.com/2025/Hashing-when-you-want-chaos/)\n\nLooking for feedback on this article I wrote recently.",
      "author": "Due_Raspberry_6269",
      "created_utc": 1746802024,
      "upvotes": 38,
      "upvote_ratio": 0.98,
      "num_comments": 10,
      "flair": "Article",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kikar2/hashing_isnt_just_for_lookups_how_randomness/"
    },
    {
      "id": "1khtocr",
      "title": "I've been watching a video explaining the simplex method for linear programming. I got to this screen, and I have a question",
      "content": "First, I watched the video several times to make sure that the lecturer in the video didn't explain the points that I didn't understand.\n\nWhat exactly is Cb? Is that something I'm supposed to know before I dive into the simplex method? And why are all the values 0? And when he determined the pivot row, he replaced the third Cb value (which was 0) with -3. Why? \n\nIt may look like a dumb point to not understand, but I'm really bad at solving linear programming problems.\n\nI humbly ask you to explain it to me like you're explaining it to a 8 yo kid.\n\nAnd have a nice day!\n\n",
      "author": "Glad_Mix_4028",
      "created_utc": 1746719938,
      "upvotes": 14,
      "upvote_ratio": 0.8,
      "num_comments": 4,
      "flair": "Help",
      "url": "https://i.redd.it/qbiw349yvkze1.png",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1khtocr/ive_been_watching_a_video_explaining_the_simplex/"
    },
    {
      "id": "1khj325",
      "title": "Is this Linear Programming Formulation of Graph Isomorphism Problem correct?",
      "content": "I was working on the TSP as a hobby and I noticed that we can express the graph isomorphism problem (GIP) as a linear programming problem but I'm not sure if this is correct because GIP is a complicated problem. You can find more details of the properties I use in this [working paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5209988).  \nFor those who want to try the model, in this [link](https://financioneroncios.wordpress.com/2025/05/08/a-new-linear-programming-formulation-of-the-graph-isomorphism-problem/) I created an example using Python and CVXPY. I recommend using a commercial solver like MOSEK, as this model has a number of variables and constraints proportional to n\\^{4}.",
      "author": "Hammercito1518",
      "created_utc": 1746684447,
      "upvotes": 29,
      "upvote_ratio": 0.9,
      "num_comments": 16,
      "flair": null,
      "url": "https://i.redd.it/0y224u3gzhze1.png",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1khj325/is_this_linear_programming_formulation_of_graph/"
    },
    {
      "id": "1kgvi2v",
      "title": "My Confusion about Addresses",
      "content": "I'm trying to better understand how variables and memory addresses work in C/C++. For example, when I declare `int a = 10;`, I know that `a` is stored somewhere in memory and has an address, like `0x00601234`. But I'm confused about what exactly is stored in RAM. Does RAM store both the address and the value? Or just the value? Since the address itself looks like a 4-byte number, I started wondering — is the address stored alongside the value? Or is the address just the position in memory, not actually stored anywhere? And when I use `&a`, how does that address get generated or retrieved if it's not saved in RAM? I’m also aware of virtual vs physical addresses and how page tables map between them, but I’m not sure how that affects this specific point about where and how addresses are stored. Can someone clarify what exactly is stored in memory when you declare a variable, and how the address works under the hood?",
      "author": "Infinite_Swimming861",
      "created_utc": 1746619052,
      "upvotes": 41,
      "upvote_ratio": 0.94,
      "num_comments": 24,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kgvi2v/my_confusion_about_addresses/"
    },
    {
      "id": "1kgw29z",
      "title": "How to carry over DSA Skills from one language to another?",
      "content": "I'm a student and fairly new to the entire DSA thing. I've been using c++ to solve basic problems. \n\nRecently i discovered that python offers simple ways to carry out things that would take me hours to code in c++. \n\nDo i just make the switch over to python or stick to c++?",
      "author": "Prudent_Spinach_5141",
      "created_utc": 1746620777,
      "upvotes": 11,
      "upvote_ratio": 0.82,
      "num_comments": 5,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kgw29z/how_to_carry_over_dsa_skills_from_one_language_to/"
    },
    {
      "id": "1kgtiir",
      "title": "Gray-Hamming Distance Fractal",
      "content": "[Gray-Hamming Distance Fractal 1..10 bits GIF](https://i.redd.it/paylde27qbze1.gif)\n\nFirst of all, I don't know whether this is really a fractal, but it looks pretty cool.  \nHere is Google Colab link where you can play with it: [Gray-Hamming Distance Fractal.ipynb](https://colab.research.google.com/drive/1qCz_4BlAmaPLS4tSU9zjPvenaUzUd9L5?usp=sharing)\n\nThe recipe:\n\n1. **Start with Integers:** Take a range of integers, say 0 to 255 (which can be represented by 8 bits).\n2. **Gray Code:** Convert each integer into its corresponding Gray code bit pattern.\n3. **Pairwise Comparison:** For every pair of Gray code bit patterns`(j, k)` calculate the **Hamming distance between these two Gray code patterns**\n4. **Similarity Value:** Convert this Hamming distance `(HD)` into a similarity value ranging from -1 to 1 using the formula: `Similarity = 1 - (2 * HD / D)`where `D` is the number of bits (e.g. 8 bits)\n   * This formula is equivalent to the cosine similarity of specific vectors. If we construct a D-dimensional vector for each Gray code pattern by summing `D` orthonormal basis vectors, where each basis vector is weighted by `+1` or `-1` according to the corresponding bit in the Gray code pattern, and then normalize the resulting sum vector to unit length (by dividing by `sqrt(D)`), the dot product (and thus cosine similarity) of any two such normalized vectors is precisely `1 - (2 * HD / D)`\n5. **Visualize:** Create a matrix where the pixel at `(j,k)` is colored based on this `Similarity`value.\n\nThe resulting image displays a distinct fractal pattern with branching, self-similar structures.\n\n[Gray-Hamming Distance Fractal 8bits](https://preview.redd.it/kr6wsx29xbze1.png?width=779&format=png&auto=webp&s=8c0b4c455a476a0fbab889cad3b2581c66df0247)\n\nI'm curious if this specific construction relates to known fractals.",
      "author": "kiockete",
      "created_utc": 1746611856,
      "upvotes": 15,
      "upvote_ratio": 0.95,
      "num_comments": 3,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kgtiir/grayhamming_distance_fractal/"
    },
    {
      "id": "1kfqvuy",
      "title": "is graph theory a good expertise in computer science",
      "content": "i really enjoy graph theory problems and the algorithms associated with them. i guess my question is, would becoming proficient in this theory be useful? i haven’t really found a branch of comp sci to “expertise” in and was looking for perspectives.",
      "author": "Full-Silver196",
      "created_utc": 1746489594,
      "upvotes": 70,
      "upvote_ratio": 0.94,
      "num_comments": 25,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kfqvuy/is_graph_theory_a_good_expertise_in_computer/"
    },
    {
      "id": "1kf9xut",
      "title": "I need an efficient data-structure to do index-based range-searches over mutable records",
      "content": "The use-case is that I want to add records with a certain weight and do random picks from the map with these weights driving the probabilities of picking a specific record. This would be easy enough to do, but these records need to be mutable and since it's going to be both very busy and very big (hundreds of millions of records), resizing the complete index on each modification is out of the question.\n\nThis structure is expected to be very big and busy.\n\n  \nSo, to put it differently: If I have the elements A, B, C and D with the (respective) relative weights of 1, 2, 3, 4, the chances of picking A will be 1:10 (10=1+2+3+4). If I then remove B, the chances of picking A will be 1:8. I'm thinking if something like this doesn't exist already (as is) I could go with some kind of cross between a b-tree and a trie, where we would have multi-level indexes, but where the reading process needs to add up the values of the keys along the way, to know if they should move sideways or deeper in the tree.",
      "author": "andras_gerlits",
      "created_utc": 1746446739,
      "upvotes": 7,
      "upvote_ratio": 0.82,
      "num_comments": 5,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kf9xut/i_need_an_efficient_datastructure_to_do/"
    },
    {
      "id": "1kewsdw",
      "title": "Computer Networking Resources",
      "content": "Hello buddies,\n\nIs there a computer networks resource that isn't actually garbage?\n\nLet me explain. I am about to graduate in Math and CS and my uni kind of failed me on the systems side. I took your typical Computer Systems - Networks - Operating Systems classes but, by luck or otherwise, these 3 were taught on a lecturer-reading-slides way. \n\nNow, about to get my diploma, I'm clueless about networks. Is there a nice book, youtube lecture series, or something, that actually teaches you networks in the same way that other courses would teach you something hands-on? Even if theoretical? Here are some examples of what I mean. \n\nAlgorithms is hands on: problem sets that asks you to proof correctness of algorithms, computing complexity, coming up with variations of algos to solve a problem. \n\nData Structures is hands on: code the structures from scratch on c++.\n\nML is hands on: get a dataset and build a model that classifies well\n\nSWE is hands on: Read an architecture pattern and code something with it\n\nMath is hands on: literally just do problem sets\n\nWhat resources is hands-ons in networking? I don't want to memorize that the TCP header is 8 bytes (or whatever size it is) without ever looking at it beyond the silly graph in your usual textbook. I want to solve some problems, code something up, do something. Kurose's book problem, skimming through them, feel more like High School trivia, though I might be wrong. Any help is most welcomed.",
      "author": "JotatD",
      "created_utc": 1746399206,
      "upvotes": 7,
      "upvote_ratio": 0.73,
      "num_comments": 3,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kewsdw/computer_networking_resources/"
    },
    {
      "id": "1kdn2o4",
      "title": "X compiler is written in X",
      "content": "I find that an _X compiler being written in X_ pretty weird, for example typescript compiler is written in typescript, go compiler is written in go, lean compiler is written in lean, C compiler is written in C\n\nExcept C, because it's almost a direct translation to hardware, so writing a simple C compiler in asm is simple then bootstrapping makes sense.\n\nBut for other high level languages, why do people bootstrap their compiler?",
      "author": "nextbite12302",
      "created_utc": 1746256268,
      "upvotes": 389,
      "upvote_ratio": 0.91,
      "num_comments": 173,
      "flair": null,
      "url": "https://i.redd.it/p3ebpbufoiye1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kdn2o4/x_compiler_is_written_in_x/"
    },
    {
      "id": "1ke1xy4",
      "title": "Flow network - residual graphs",
      "content": "I’m sorry if this isn’t the correct place to ask such a question but I didn’t this exactly breaking the rules.  I’m currently studying for my algorithms final tomorrow and I’ve been conceptually struggling to understand the role of the residual graph and residual paths in finding the max-flow. \n\nIn the graph attached, when using the Ford Fulkerson algorithm with DFS, in the worst case a flow of 1 is pushed through the augmenting path repeatedly in an oscillating manner. What I’m struggling to understand is why, after the very first time that the augmenting path is found and a flow of 1 is pushed through it, causing the flow to equal capacity through the middle edge, we are still able to find the same augmenting path again and again and pass flow through it.  \n\nI’d really appreciate any help! Thanks a lot. ",
      "author": "Anonymous-badger79",
      "created_utc": 1746302961,
      "upvotes": 8,
      "upvote_ratio": 1.0,
      "num_comments": 2,
      "flair": "Help",
      "url": "https://i.redd.it/5afaynaajmye1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1ke1xy4/flow_network_residual_graphs/"
    },
    {
      "id": "1kdwc4c",
      "title": "Relation between API, driver and firmware",
      "content": "What is the relation between API, driver and firmware? From what I understand API is the intermediate between the application and the driver, the driver gives the low level instructions and firmware does what?",
      "author": "Tranomial_2",
      "created_utc": 1746288122,
      "upvotes": 5,
      "upvote_ratio": 0.73,
      "num_comments": 3,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kdwc4c/relation_between_api_driver_and_firmware/"
    },
    {
      "id": "1kcigvx",
      "title": "Ford-Fulkerson Algorithm: A Step-by-Step Guide to Max Flow",
      "content": null,
      "author": "teivah",
      "created_utc": 1746130557,
      "upvotes": 6,
      "upvote_ratio": 0.88,
      "num_comments": 1,
      "flair": null,
      "url": "https://www.thecoder.cafe/p/ford-fulkerson",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kcigvx/fordfulkerson_algorithm_a_stepbystep_guide_to_max/"
    },
    {
      "id": "1kc83d1",
      "title": "How to count without the side effect caused by float precision of decimal numbers ?",
      "content": "Given two arbitrary vectors, which represent a bounding box in 3D space . They represent the leftbottom and the righttop corners of a box geometry . My question is , I want to voxelize this bounding box, but I can't get a correct number of total number of boxes .\n\nTo elaborate : I want to represent this bounding volume with several little cubes of constant size . And they will be placed along each axis with different amounts per axis. This technically would be easy but soon I encountered the problem of float precision . As decimal numbers are represented with negative powers, you have to fit the numerical value . Binary representation cannot represent it easily . It's like binary tree that you divide the whole tree into \"less than 0.5\" and \"greater than 0.5\" . After that , you divide each parts into 0.25 and 0.75. You repeat this process and finally get an approximate value .\n\nThe problem is : ***ceil((righttop.x-leftbottom.x)/cubesize)*** outputs 82 while ***ceil(righttop.x/cubesize)-ceil(leftbottom.x/cubesize)*** outputs 81 because ***(righttop.x-leftbottom.x)/cubesize*** equals to 81.000001 which is ceiled to 82, while I was expecting it to be ***ceil(81.000001)==81*** .\n\nHow should you calculate it in this case ?",
      "author": "Significant-Gap8284",
      "created_utc": 1746104331,
      "upvotes": 10,
      "upvote_ratio": 0.92,
      "num_comments": 13,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kc83d1/how_to_count_without_the_side_effect_caused_by/"
    },
    {
      "id": "1kbo001",
      "title": "I built a toy to help learn about arrays and pointers",
      "content": "Sometimes, I get sad that most of what I build are just metaphors for electrons occupying different spaces--so I start picturing tactile representations.  Here is one I designed in Fusion for Arrays and pointers. \n\nIt helped with explaining the concept to my 10 year old--although it didn't much help with the \"but why?\" question.",
      "author": "AdventurousTown4144",
      "created_utc": 1746038512,
      "upvotes": 174,
      "upvote_ratio": 0.95,
      "num_comments": 24,
      "flair": null,
      "url": "https://www.reddit.com/gallery/1kbo001",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1kbo001/i_built_a_toy_to_help_learn_about_arrays_and/"
    },
    {
      "id": "1kbmsen",
      "title": "From Data to Display: How Computers Present Images",
      "content": "Most of us use technological devices daily, and they're an indispensable part of our lives. A few decades ago, when the first computer came up, the screen only displayed black and white colors. Nowadays, from phones to computers to technical devices, the colorful display is what we take for granted. But there is one interesting question from a technical perspective: if the computer can only understand zeros and ones, then how can a colorful image be displayed on our screen? In this blog post, we will try to address this fundamental question and walk through a complete introduction to the image rendering pipeline, from an image stored in memory to being displayed on the screen.\n\nhttps://preview.redd.it/skp1z5eby3ye1.png?width=1416&format=png&auto=webp&s=b540e7f45956e2fd55630555fb25105e5499dab2\n\n[https://learntocodetogether.com/image-from-memory-to-display/](https://learntocodetogether.com/image-from-memory-to-display/)",
      "author": "vannam0511",
      "created_utc": 1746035521,
      "upvotes": 9,
      "upvote_ratio": 0.84,
      "num_comments": 2,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kbmsen/from_data_to_display_how_computers_present_images/"
    },
    {
      "id": "1kaz67b",
      "title": "About how many bits can all the registers in a typical x86 CPU hold?",
      "content": "I know you can't necessarily actually access each one, but I was curious how many registers there are in a typical x86 processor (let's say a 4 core i7 6820 hq, simply cause it's what I have). I've only found some really rough guestimates of how many registers there are from Google, and nothing trying to actually find out how big they are (I don't know if they're all the same size or if some are smaller). Also, I was just curious which has more space, the registers in my CPU or a zx spectrums ram, because just by taking the number this thread ( https://www.reddit.com/r/programming/comments/k3wckj/how_many_registers_does_an_x8664_cpu_have/ )suggests and multiplying it by 64 then 4 you actually get a fairly similar value to the 16kb a spectrum has",
      "author": "spaciousputty",
      "created_utc": 1745961260,
      "upvotes": 26,
      "upvote_ratio": 0.85,
      "num_comments": 22,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kaz67b/about_how_many_bits_can_all_the_registers_in_a/"
    },
    {
      "id": "1kb3bea",
      "title": "Is Linear Probing Really that Bad of a Solution for Open-Addressing?",
      "content": "I've been watching several lectures on YouTube about open addressing strategies for hash tables. They always focus heavily on the number of probes without giving much consideration to cache warmth, which leads to recommending scattering techniques like double hashing instead of the more straightforward linear probing. Likewise it always boils down to probability theory instead of hard wall clock or cpu cycles.\n\nFurthermore I caught an awesome talk on the cppcon channel from a programmer working in Wall Street trading software, who eventually concluded that linear searches in an array performed better in real life for his datasets. This aligns with my own code trending towards simpler array based solutions, but I still feel the pull of best case constant time lookups that hash tables promise.\n\nI'm aware that I should be deriving my solutions based on data set and hardware, and I'm currently thinking about how to approach quantitative analysis for strategy options and tuning parameters (eg. rehash thresholds) - but i was wondering if anyone has good experience with a hash table that degrades to linear search after a single probe failure? It seems to offer the best of both worlds.\n\nAny good blog articles or video recommendations on either this problem set or related experiment design and data analysis? Thanks.",
      "author": "Star_eyed_wonder",
      "created_utc": 1745972524,
      "upvotes": 11,
      "upvote_ratio": 0.93,
      "num_comments": 5,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kb3bea/is_linear_probing_really_that_bad_of_a_solution/"
    },
    {
      "id": "1kaigyx",
      "title": "What are the Implications of P=NP?",
      "content": "I am trying to write a sci-fi thriller where in 2027, there are anomalies in the world which is starting to appear because someone proves P=NP in specific conditions and circumstances and this should have massive consequences, like a ripple effect in the world. \nI just want to grasp the concept better and understand implications to write this setting better. \nI was thinking maybe one of the characters \"solves\" the Hodge conjecture in their dream and claims they could just \"see\" it ( which btw because a scenario where P=NP is developing) and this causes a domino effect of events. \n\nI want to understand how to \"show\" Or depict it in fiction, for which I need a better grasp\n\n thanks in advance for helping me out. ",
      "author": "Yah_Ruach",
      "created_utc": 1745913003,
      "upvotes": 23,
      "upvote_ratio": 0.63,
      "num_comments": 71,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1kaigyx/what_are_the_implications_of_pnp/"
    },
    {
      "id": "1ka1y8r",
      "title": "How do Single Core Processors Handle Concurrent Processes?",
      "content": "I watched some videos on YouTube and found out that programs and processes often don't use the CPU the entire time. A process will need the CPU for \"CPU bursts\" but needs a different resource when it makes a system call.\n\nSome OS like MS-DOS were non-preemptive and waited for a process to finish its CPU burst before continue to the next one. Aside from not being concurrent if one process was particularly CPU hungry, if it had an infinite loop, this would cause process starvation. More sophisticated ones like Windows 95 and Mac OS would eventually stop a process using the CPU and then move on to another process. So by rapidly switching between multiple processes, the CPU can handle concurrent processes.\n\nMy question is how does the processor determine what is a good time to kick out a still running process? If each process is limited to 3 milliseconds, then most of the CPU time is spent swapping between processes and not actually running them. If it waits 3000 milliseconds before swapping, then the illusion of concurrently running programs is lost. Is the maximum time per process CPU (hardware) dependent? OS (Software) dependent? If it is a limit per process of each CPU, does the manufacturer publish the limit?",
      "author": "ShadowGuyinRealLife",
      "created_utc": 1745862880,
      "upvotes": 20,
      "upvote_ratio": 0.75,
      "num_comments": 44,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ka1y8r/how_do_single_core_processors_handle_concurrent/"
    },
    {
      "id": "1k94pqk",
      "title": "What happens if P=NP?",
      "content": "No I don’t have a proof I was just wondering",
      "author": "Usual-Letterhead4705",
      "created_utc": 1745762684,
      "upvotes": 125,
      "upvote_ratio": 0.89,
      "num_comments": 48,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k94pqk/what_happens_if_pnp/"
    },
    {
      "id": "1k93ewk",
      "title": "Computer Science book that will lead to insights into various Computer Systems?",
      "content": "Is there a book out there that would provide an overview of all CS that would come in handy when trying to understand things like containers, network architecture, python scripts, database replication, devops, etc? I was thinking about going through Nand2Tetris but that seems like it might be more low-level than I'd need to get the information I'm looking for. Unless you think a computer architecture and systems programming book like that would prove to be useful. Thank you for your help.",
      "author": "Tall_Telephone_9579",
      "created_utc": 1745758844,
      "upvotes": 16,
      "upvote_ratio": 0.86,
      "num_comments": 10,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k93ewk/computer_science_book_that_will_lead_to_insights/"
    },
    {
      "id": "1k8imyd",
      "title": "Transition to system programming and distributed systems",
      "content": "I've a background in full stack development and smart contract development. But it's not fulfilling for me because I love difficult tasks and challenges, and what I was doing feel really shallow. \n\nMy goal is to become a good systems programmer as well as distributed systems engineer. But I lack necessary skills to achieve my goals because my fundamentals aren't strong.\n\nSo I decided to read \"Code: Hidden Language\" by charles petzold, and after that I want to complete nand2tetris. I'll jump into C language, will create some projects, and then will learn Rust. \n\nTo become a good engineer, I think it's better if you have solid basic concepts. That's why I started to read the book and will follow the course.\n\nI want to do it full-time because it will be done sooner and without any distraction. Also context switching is a huge problem for me. So I want to focus completely on this roadmap. \n\nThe question is, am I missing something? Am I overthinking it? Is it a good roadmap? ",
      "author": "keen-hamza",
      "created_utc": 1745689321,
      "upvotes": 17,
      "upvote_ratio": 0.9,
      "num_comments": 4,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k8imyd/transition_to_system_programming_and_distributed/"
    },
    {
      "id": "1k8i5iw",
      "title": "Resources on combinatorics or discrete math in general",
      "content": "My ultamite goal is to be good at DSA. So, I'm trying to learn combinatorics from scratch, i have no idea what does it mean so far. I heard it's really important for my cs education. How to start? any courses or books that start from scratch and then dive deep. Are there any prerequisites i should learn before getting started with it? should i start with proofs and discrete math, set theory before it?",
      "author": "Mohammed1jassem",
      "created_utc": 1745688073,
      "upvotes": 7,
      "upvote_ratio": 0.82,
      "num_comments": 4,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k8i5iw/resources_on_combinatorics_or_discrete_math_in/"
    },
    {
      "id": "1k7q893",
      "title": "What,s actually in free memory!",
      "content": "So let’s say I bought a new SSD and installed it into a PC. Before I format it or install anything, what’s really in that “free” or “empty” space? Is it all zeros? Is it just undefined bits? Does it contain null? Or does it still have electrical data from the factory that we just can’t see?",
      "author": "Canon_07",
      "created_utc": 1745600980,
      "upvotes": 41,
      "upvote_ratio": 0.92,
      "num_comments": 27,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k7q893/whats_actually_in_free_memory/"
    },
    {
      "id": "1k70ncf",
      "title": "My Computer Science final said CDs are not storage?",
      "content": "Aren’t they? They store files by definition…the question was “blue ray discs and CDs are examples of storage devices” I selected true but got the question wrong. Worth messaging teacher? I also was asked if a smart watch was a Ubiquitous computer and said yes but that also came back as wrong. After the test I looked up both things and it says I’m correct. Are these debatable topics? Could my teacher have a reason or did I miss something in the way it was asked? \n\nIs this worth sending a message to him for? \n\n\nEdit: I did message him for clarity with the understanding I may be incorrect based on technicalities and opinion! I actually am really enjoying this post now because it’s brought up a rather interesting debate on something I didn’t think too deeply about! \n\n\nUpdate a few days later or a week idk: My teacher responded to my message that I did miss the media vs device distinction. However he actually did change my grade because he agreed that a wearable watch could be classified as Ubiquitous. I would like to think messaging him was worth it! Thank you to everyone who commented and contributed to the discussion :) ",
      "author": "Odd-Boysenberry-9454",
      "created_utc": 1745521929,
      "upvotes": 417,
      "upvote_ratio": 0.95,
      "num_comments": 199,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k70ncf/my_computer_science_final_said_cds_are_not_storage/"
    },
    {
      "id": "1k77ekx",
      "title": "(Why) are compilers course practicums especially difficult?",
      "content": "In more than one (good) academic institution I've taken a compilers course at, students or professors have said \"this course is hard,\" and they're not wrong.\n\nI have no doubt it's one of the best skills you can acquire in your career. I just wonder if they are inherently more difficult than other practicums (e.g.  databases, operating systems, networks).\n\nAre there specific hurdles when constructing a compiler that transcends circumstantial factors like the institution, professor that are less of a problem with other areas of computer science?",
      "author": "sarnobat",
      "created_utc": 1745539668,
      "upvotes": 48,
      "upvote_ratio": 0.93,
      "num_comments": 26,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k77ekx/why_are_compilers_course_practicums_especially/"
    },
    {
      "id": "1k731bd",
      "title": "Computer science books and roadmaps",
      "content": "Hi all, I want to achieve a deeper understanding of computer science that goes beyond software eng. Could you share books that I should read and are considered “bibles” , roadmaps and suggestions? I am a physicist working at the moment as data eng",
      "author": "ubiond",
      "created_utc": 1745527774,
      "upvotes": 21,
      "upvote_ratio": 0.92,
      "num_comments": 30,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k731bd/computer_science_books_and_roadmaps/"
    },
    {
      "id": "1k64jxe",
      "title": "Computer science theory wins you’ve actually used for prep",
      "content": "We all learned heaps of algorithm / automata theory, but how often do you really deploy it?\n\nMy recent win: turned a gnarly string‑search bug into a clean Aho‑Corasick automaton cut runtime from 45 s ➜ 900 ms.  \n\nA teammate used max‑flow / min‑cut to optimize a supply‑chain model, saving the client \\~$40 k/mo.\n\n\n\nDrop your stories (and what course prepped you). Bonus points if the professor swore “you’ll use this someday”… and they were right.  \n\n",
      "author": "nvntexe",
      "created_utc": 1745428198,
      "upvotes": 198,
      "upvote_ratio": 0.98,
      "num_comments": 35,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k64jxe/computer_science_theory_wins_youve_actually_used/"
    },
    {
      "id": "1k69z70",
      "title": "Data reconstruction from machine learning models via inverse estimation and Bayesian inference",
      "content": null,
      "author": "mandelbrot1981",
      "created_utc": 1745441312,
      "upvotes": 8,
      "upvote_ratio": 0.91,
      "num_comments": 1,
      "flair": null,
      "url": "https://www.nature.com/articles/s41598-025-96215-z",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1k69z70/data_reconstruction_from_machine_learning_models/"
    },
    {
      "id": "1k4wka0",
      "title": "Is this correct? If not, how would you make it correct?",
      "content": null,
      "author": "VeganTheStallion",
      "created_utc": 1745292059,
      "upvotes": 166,
      "upvote_ratio": 0.84,
      "num_comments": 71,
      "flair": null,
      "url": "https://i.redd.it/4h3mf4qc1bwe1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1k4wka0/is_this_correct_if_not_how_would_you_make_it/"
    },
    {
      "id": "1k4usm2",
      "title": "Did we miss something focusing so much on Turing/Von Neumann style computers?",
      "content": "I know that quantum computers have been around for a little while, but that's not what I'm talking about.\nI'm talking about perhaps an alternative classical computer. What would we have come up with if we didn't have Turing or Von Neumann?\nWas there a chance it'd be better or worse?\nI know Turing was one monumentally brilliant man, I'm just not sure if we could've done any better.\n\nedit: Why are you guys upvoting this. I've come to realize this is a very stupid question.",
      "author": "[deleted]",
      "created_utc": 1745286554,
      "upvotes": 158,
      "upvote_ratio": 0.94,
      "num_comments": 72,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k4usm2/did_we_miss_something_focusing_so_much_on/"
    },
    {
      "id": "1k4i4az",
      "title": "Wild how many people in a OpenAI subreddit thread still think LLMs are sentient, do they even know how transformers work?",
      "content": null,
      "author": "DerpDerper909",
      "created_utc": 1745253983,
      "upvotes": 155,
      "upvote_ratio": 0.94,
      "num_comments": 47,
      "flair": "Discussion",
      "url": "/r/OpenAI/comments/1k48t0z/the_amount_of_people_in_this_sub_that_think/",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1k4i4az/wild_how_many_people_in_a_openai_subreddit_thread/"
    },
    {
      "id": "1k4njwn",
      "title": "ELI5: What is OAuth?",
      "content": "So I was reading about OAuth to learn it and have created this explanation. It's basically a few of the best I have found merged together and rewritten in big parts. I have also added a super short summary and a code example. Maybe it helps one of you :-) This is the [repo][ref2].\n\n# OAuth Explained\n\n## The Basic Idea\n\nLet’s say LinkedIn wants to let users import their Google contacts.\n\nOne obvious (but terrible) option would be to just ask users to enter their Gmail email and password directly into LinkedIn. But giving away your actual login credentials to another app is a huge security risk.\n\nOAuth was designed to solve exactly this kind of problem.\n\nNote: So OAuth solves an authorization problem! Not an authentication problem. See [here][ref1] for the difference.\n\n## Super Short Summary\n\n- User clicks “Import Google Contacts” on LinkedIn\n- LinkedIn redirects user to Google’s OAuth consent page\n- User logs in and approves access\n- Google redirects back to LinkedIn with a one-time code\n- LinkedIn uses that code to get an access token from Google\n- LinkedIn uses the access token to call Google’s API and fetch contacts\n\n## More Detailed Summary\n\nSuppose LinkedIn wants to import a user’s contacts from their Google account.\n\n1. LinkedIn sets up a Google API account and receives a client_id and a client_secret\n   - So Google knows this client id is LinkedIn\n2. A user visits LinkedIn and clicks \"Import Google Contacts\"\n3. LinkedIn redirects the user to Google’s authorization endpoint:\n   https://accounts.google.com/o/oauth2/auth?client_id=12345&redirect_uri=https://linkedin.com/oauth/callback&scope=contacts\n\n- client_id is the before mentioned client id, so Google knows it's LinkedIn\n- redirect_uri is very important. It's used in step 6\n- in scope LinkedIn tells Google how much it wants to have access to, in this case the contacts of the user\n\n4. The user will have to log in at Google\n5. Google displays a consent screen: \"LinkedIn wants to access your Google contacts. Allow?\" The user clicks \"Allow\"\n6. Google generates a one-time authorization code and redirects to the URI we specified: redirect_uri. **It appends the one-time code as a URL parameter**.\n   - So the URL could be https://linkedin.com/oauth/callback?code=one_time_code_xyz\n7. Now, LinkedIn makes a server-to-server request (not a redirect) to Google’s token endpoint and receive an access token (and ideally a refresh token)\n8. **Finished**. Now LinkedIn can use this access token to access the user’s Google contacts via Google’s API\n\n---\n\n**Question:**\n_Why not just send the access token in step 6?_\n\n**Answer:** To make sure that the requester is actually LinkedIn. So far, all requests to Google have come from the user’s browser, with only the client_id identifying LinkedIn. Since the client_id isn’t secret and could be guessed by an attacker, Google can’t know for sure that it's actually LinkedIn behind this. In the next step, LinkedIn proves its identity by including the client_secret in a server-to-server request.\n\n## Security Note: Encryption\n\nOAuth 2.0 does **not** handle encryption itself. It relies on HTTPS (SSL/TLS) to secure sensitive data like the client_secret and access tokens during transmission.\n\n## Security Addendum: The state Parameter\n\nThe state parameter is critical to prevent cross-site request forgery (CSRF) attacks. It’s a unique, random value generated by the third-party app (e.g., LinkedIn) and included in the authorization request. Google returns it unchanged in the callback. LinkedIn verifies the state matches the original to ensure the request came from the user, not an attacker.\n\n## OAuth 1.0 vs OAuth 2.0 Addendum:\n\nOAuth 1.0 required clients to cryptographically sign every request, which was more secure but also much more complicated. OAuth 2.0 made things simpler by relying on HTTPS to protect data in transit, and using bearer tokens instead of signed requests.\n\n## Code Example: OAuth 2.0 Login Implementation\n\nBelow is a standalone Node.js example using Express to handle OAuth 2.0 login with Google, storing user data in a SQLite database.\n\n```javascript\nconst express = require(\"express\");\nconst axios = require(\"axios\");\nconst sqlite3 = require(\"sqlite3\").verbose();\nconst crypto = require(\"crypto\");\nconst jwt = require(\"jsonwebtoken\");\nconst jwksClient = require(\"jwks-rsa\");\n\nconst app = express();\nconst db = new sqlite3.Database(\":memory:\");\n\n// Initialize database\ndb.serialize(() => {\n  db.run(\n    \"CREATE TABLE users (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, email TEXT)\"\n  );\n  db.run(\n    \"CREATE TABLE federated_credentials (user_id INTEGER, provider TEXT, subject TEXT, PRIMARY KEY (provider, subject))\"\n  );\n});\n\n// Configuration\nconst CLIENT_ID = process.env.GOOGLE_CLIENT_ID;\nconst CLIENT_SECRET = process.env.GOOGLE_CLIENT_SECRET;\nconst REDIRECT_URI = \"https://example.com/oauth2/callback\";\nconst SCOPE = \"openid profile email\";\n\n// JWKS client to fetch Google's public keys\nconst jwks = jwksClient({\n  jwksUri: \"https://www.googleapis.com/oauth2/v3/certs\",\n});\n\n// Function to verify JWT\nasync function verifyIdToken(idToken) {\n  return new Promise((resolve, reject) => {\n    jwt.verify(\n      idToken,\n      (header, callback) => {\n        jwks.getSigningKey(header.kid, (err, key) => {\n          callback(null, key.getPublicKey());\n        });\n      },\n      {\n        audience: CLIENT_ID,\n        issuer: \"https://accounts.google.com\",\n      },\n      (err, decoded) => {\n        if (err) return reject(err);\n        resolve(decoded);\n      }\n    );\n  });\n}\n\n// Generate a random state for CSRF protection\napp.get(\"/login\", (req, res) => {\n  const state = crypto.randomBytes(16).toString(\"hex\");\n  req.session.state = state; // Store state in session\n  const authUrl = `https://accounts.google.com/o/oauth2/auth?client_id=${CLIENT_ID}&redirect_uri=${REDIRECT_URI}&scope=${SCOPE}&response_type=code&state=${state}`;\n  res.redirect(authUrl);\n});\n\n// OAuth callback\napp.get(\"/oauth2/callback\", async (req, res) => {\n  const { code, state } = req.query;\n\n  // Verify state to prevent CSRF\n  if (state !== req.session.state) {\n    return res.status(403).send(\"Invalid state parameter\");\n  }\n\n  try {\n    // Exchange code for tokens\n    const tokenResponse = await axios.post(\n      \"https://oauth2.googleapis.com/token\",\n      {\n        code,\n        client_id: CLIENT_ID,\n        client_secret: CLIENT_SECRET,\n        redirect_uri: REDIRECT_URI,\n        grant_type: \"authorization_code\",\n      }\n    );\n\n    const { id_token } = tokenResponse.data;\n\n    // Verify ID token (JWT)\n    const decoded = await verifyIdToken(id_token);\n    const { sub: subject, name, email } = decoded;\n\n    // Check if user exists in federated_credentials\n    db.get(\n      \"SELECT * FROM federated_credentials WHERE provider = ? AND subject = ?\",\n      [\"https://accounts.google.com\", subject],\n      (err, cred) => {\n        if (err) return res.status(500).send(\"Database error\");\n\n        if (!cred) {\n          // New user: create account\n          db.run(\n            \"INSERT INTO users (name, email) VALUES (?, ?)\",\n            [name, email],\n            function (err) {\n              if (err) return res.status(500).send(\"Database error\");\n\n              const userId = this.lastID;\n              db.run(\n                \"INSERT INTO federated_credentials (user_id, provider, subject) VALUES (?, ?, ?)\",\n                [userId, \"https://accounts.google.com\", subject],\n                (err) => {\n                  if (err) return res.status(500).send(\"Database error\");\n                  res.send(`Logged in as ${name} (${email})`);\n                }\n              );\n            }\n          );\n        } else {\n          // Existing user: fetch and log in\n          db.get(\n            \"SELECT * FROM users WHERE id = ?\",\n            [cred.user_id],\n            (err, user) => {\n              if (err || !user) return res.status(500).send(\"Database error\");\n              res.send(`Logged in as ${user.name} (${user.email})`);\n            }\n          );\n        }\n      }\n    );\n  } catch (error) {\n    res.status(500).send(\"OAuth or JWT verification error\");\n  }\n});\n\napp.listen(3000, () => console.log(\"Server running on port 3000\"));\n```\n\n[ref1]: https://stackoverflow.com/questions/6556522/authentication-versus-authorization\n[ref2]: https://github.com/LukasNiessen/oauth-explained",
      "author": "trolleid",
      "created_utc": 1745266844,
      "upvotes": 25,
      "upvote_ratio": 0.87,
      "num_comments": 2,
      "flair": "Article",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k4njwn/eli5_what_is_oauth/"
    },
    {
      "id": "1k4fuc9",
      "title": "Doubt in Dsa",
      "content": "Guys, while traversing a directed graph using BFS or DFS, some nodes may not be reachable. What should we do in that case? Is it okay to leave ?",
      "author": "kashHere",
      "created_utc": 1745248059,
      "upvotes": 29,
      "upvote_ratio": 0.91,
      "num_comments": 8,
      "flair": "Help",
      "url": "https://www.reddit.com/gallery/1k4fuc9",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1k4fuc9/doubt_in_dsa/"
    },
    {
      "id": "1k4f7le",
      "title": "Swift or Python for teaching 16+ Programming?",
      "content": "I come to teaching FE from a React/Node/PHP background and have been looking at Swift recently. Its ability to explicitly type variables seems to be a big win over the current A'Level favourite of Python which is hideously loosely typed. As most of the examining boards do not mandate a specific language, I'm wondering if I shouldn't be arguing for the introduction of Swift as a language for us to teach across multiple platforms, and even easily incorporate UI Apps for students to see beynd the command line. What do other teachers of programming think?",
      "author": "PeterPook",
      "created_utc": 1745246513,
      "upvotes": 8,
      "upvote_ratio": 0.84,
      "num_comments": 34,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k4f7le/swift_or_python_for_teaching_16_programming/"
    },
    {
      "id": "1k411lr",
      "title": "Typical computer speeds",
      "content": "Hi everyone,\n\nI understand that most modern processors typically run at speeds between 2.5 and 4 GHz. Given this, I'm curious why my computer sometimes takes a relatively long time to process certain requests. What factors, aside from the CPU clock speed, could be contributing to these delays?",
      "author": "AtlasManuel",
      "created_utc": 1745196474,
      "upvotes": 9,
      "upvote_ratio": 0.62,
      "num_comments": 49,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k411lr/typical_computer_speeds/"
    },
    {
      "id": "1k3vp73",
      "title": "Byzantine Fault Tolerance: How Computers Trust Each Other When They Shouldn't",
      "content": "Wanted to share this cool concept called Byzantine Fault Tolerance (BFT). It tackles one of distributed computing's toughest challenges: how do computers reach agreement when some nodes might be sending contradictory information to different parts of the system? Named after the Byzantine Generals' Problem, these algorithms ensure systems keep working correctly even when up to a third of nodes are compromised or malfunctioning. Air traffic control systems use BFT principles to make critical decisions when some radar inputs might be giving false readings. Distributed databases rely on BFT for syncing state. Same thing with blockchains. The list goes on... \n\nOne game changer was the Practical Byzantine Fault Tolerance algorithm developed in 1999 (https://pmg.csail.mit.edu/papers/osdi99.pdf), which made these systems actually implementable in the real world. Before that, the communication overhead was too massive to be useful. Now BFT principles protect everything from cloud databases to financial networks, creating systems that don't just detect failures but can continue operating reliably through them. \n\nFor more on this by the legend leslie lamport himself: [https://lamport.azurewebsites.net/pubs/byz.pdf](https://lamport.azurewebsites.net/pubs/byz.pdf)",
      "author": "aeronauticator",
      "created_utc": 1745180836,
      "upvotes": 15,
      "upvote_ratio": 0.95,
      "num_comments": 1,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k3vp73/byzantine_fault_tolerance_how_computers_trust/"
    },
    {
      "id": "1k3ep22",
      "title": "Any computer networking textbooks you'd recommend for teaching to highschool?",
      "content": "Pretty much what the title says. I need something the kids can read from and not run away as soon as they see the first acronym.",
      "author": "ZenithCrests",
      "created_utc": 1745123956,
      "upvotes": 8,
      "upvote_ratio": 0.8,
      "num_comments": 5,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k3ep22/any_computer_networking_textbooks_youd_recommend/"
    },
    {
      "id": "1k23e4m",
      "title": "fully understanding computers and internet",
      "content": "hi, all. I would like to fully understand computers and internet and how it all functions and not just on a surface level like what each part does, or something like that. I want to be able to break it down until I can't anymore, only because there isnt really anything left, not because of limited knowledge; and I don't really know where to start, hence my post here: so I'm looking for directions.\nIt would be great if anyone could give me a list of materials and whatever other word of advice, thanks :D",
      "author": "Valkyyri",
      "created_utc": 1744978231,
      "upvotes": 59,
      "upvote_ratio": 0.8,
      "num_comments": 75,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k23e4m/fully_understanding_computers_and_internet/"
    },
    {
      "id": "1k1shwf",
      "title": "Why do video game engines use floats rather than ints (details of question in body)",
      "content": "So the way it was explained to me, floats are prefered because they allow greater range, which makes a lot of sense.\n\nReasonably, in most games I imagine that the slowest an object can move is the equivalent of roughly 1 mm/second, and the fastest is equivalent to probably maximum bullet velocity,  roughly 400 meter/second, i.e. 400,000 mm/second. This suggests that integers from 1 to 400,000 cover all reasonable speed ranges, i.e. 19 bits, and even if we allowed much greater ranges of numbers for other quantities, it is not immediately obvious to me why one would ever exceed a 32-bit signed integer, let alone a 64-bit int.\n\nI'm guessing that this means that there are other considerations at play that I'm not taking into account. What am I missing folks?\n\n  \nEDIT: THANK EVERYBODY FOR THE DETAILED RESPONSES!",
      "author": "JewishKilt",
      "created_utc": 1744937509,
      "upvotes": 172,
      "upvote_ratio": 0.83,
      "num_comments": 172,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k1shwf/why_do_video_game_engines_use_floats_rather_than/"
    },
    {
      "id": "1k18ehj",
      "title": "Is systems biology mostly computer science?",
      "content": "Hello, I was wondering what's the difference between systems biology (not expiremental) and computational biology/bioinformatics. I have read that systems biology is computational and mathematical modelling? Do you spend most of the time coding and troubleshooting code?  Is mathematical biology actually more math modelling and less coding?",
      "author": "ilovemedicine1233",
      "created_utc": 1744881119,
      "upvotes": 34,
      "upvote_ratio": 0.96,
      "num_comments": 23,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1k18ehj/is_systems_biology_mostly_computer_science/"
    },
    {
      "id": "1jz4oqu",
      "title": "Computer Science Roadmap",
      "content": "https://roadmap.sh/computer-science\n\nWhat do you think about this roadmap? I feel like this isn't enough. Because I couldn't see lessons for math, physics, computer architecture, operating systems etc. I'm new to this, so I accept any kind of comments :D",
      "author": "Physical-Vast7175",
      "created_utc": 1744651881,
      "upvotes": 53,
      "upvote_ratio": 0.88,
      "num_comments": 26,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jz4oqu/computer_science_roadmap/"
    },
    {
      "id": "1jy8t8g",
      "title": "relating all concepts you learn from different streams of science",
      "content": "im a freshman in CS and currently i have five classes OOP(java), Database systems, Digital Logic design, Discrete Mathematics and Calculus. in last sem we did C++ fundamentals, ICT, precalc. the thing is i was wondering if its possible to connect all of the concepts im learning or have learned. its so confusing idk how to explain but basically we have concepts in Discrete Maths and DLD which overlap but i cannot figure out a way to do it. like create a single interrelated network /web of all the interrelated stem fields where i can add new concepts as i learn them. kind of like a murdermap. i just wanted to know if itd be possible or if anyone has tried doing it or if its too stupid of an idea ",
      "author": "Ordinary-Sort1304",
      "created_utc": 1744554253,
      "upvotes": 17,
      "upvote_ratio": 0.88,
      "num_comments": 8,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jy8t8g/relating_all_concepts_you_learn_from_different/"
    },
    {
      "id": "1jy0nay",
      "title": "Any application of Signals and Systems?",
      "content": "I am interested in learning more about the subject of image processing/computational imaging. For reference, I have/am planning to take college courses in Computer Graphics, Computer Vision, and ML. Is there any use for me to take a semester to learn the math of Signals and Systems, where I will not (formally) learn specifically about Digital Signal Processing? It's a field I'm curious about, but not dead set on. And I'd rather not waste my time on something if I likely am not going to be using it ever/learning a lot more information (Analog DS) than I need to.\n\nWhat background would I want to know for Image Processing. Would it need to be a lot of math like S&S?\n\nGoing to say (for the mods) that I hope this doesn't go against rule 3 since it's more about the application of a subject in CS than classes specifically.",
      "author": "MTsterfri",
      "created_utc": 1744522744,
      "upvotes": 13,
      "upvote_ratio": 0.89,
      "num_comments": 7,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jy0nay/any_application_of_signals_and_systems/"
    },
    {
      "id": "1jxpua9",
      "title": "A Computational Graph builder for circuit evaluation and constraint checking",
      "content": "Built a library for constructing computational graphs that allows you to represent any function or computational circuit as a graph and run evaluations on it or specific constraint checks. This is very relevant in the area of verifiable computation and zero knowledge proofs. A lot of the algorithms in that realm usually require you to represent whatever function/computation you're evaluating as a graph which you can then evaluate constraints, etc. I've been wanting to write a bunch of these proof systems from scratch so built this as a primitive that I can use to make things easier.\n\nThe algorithm I wrote creates a level for each arithmetic operation starting from the input nodes. The evaluation and constraint checking is then performed in a sorted manner for each level, and is parallelized across all the nodes in a given level. Constraints are also checked once all the nodes involved in that constraint have computed values. I wrote it in Rust :) \n\nI provided a few examples in the readme: [https://github.com/AmeanAsad/comp-graph/blob/main/README.md](https://github.com/AmeanAsad/comp-graph/blob/main/README.md)\n\n\n\n  \n\n\n",
      "author": "aeronauticator",
      "created_utc": 1744487600,
      "upvotes": 17,
      "upvote_ratio": 0.95,
      "num_comments": 7,
      "flair": null,
      "url": "https://github.com/AmeanAsad/comp-graph",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1jxpua9/a_computational_graph_builder_for_circuit/"
    },
    {
      "id": "1jxopcz",
      "title": "Why electrons flow from the N-semiconductor to a P-semiconductor?",
      "content": "Suppose we have an NP-semiconductor. From what I understand, electrons flow to fill in the holes in P. That creates a potential barrier, that prevents further electron flow, from N to P. Since at the barrier, N becomes positively charged and P becomes negatively charged, why aren't electrons flowing back? I think one way to answer the question is to answer the following: why do electrons even want to fill those holes?",
      "author": "Valuable-Glass1106",
      "created_utc": 1744484573,
      "upvotes": 13,
      "upvote_ratio": 0.79,
      "num_comments": 8,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jxopcz/why_electrons_flow_from_the_nsemiconductor_to_a/"
    },
    {
      "id": "1jwujre",
      "title": "Best data structure for representing a partially ordered set (POSET) or lattices",
      "content": "So I have recently been diving into refinement calculus because I found it to be really interesting and has potential for a lot of things, as I was going through the famous book , the chapter starts with a theoretical foundations on lattice theory, which forms the groundwork for later work. To further my understanding of them I wanted to implement them in code however iam not sure exactly what is the best way to represent them, since lattices are simply posets (partially ordered sets) but with extra conditions like bottom and top , I figured if I efficiently represent posets I can then extend the implementation to lattices, however even that seems to have so many different options, like adjacency matrix ,DAG (directed asyclic graphs), many other stuff. If anyone has any idea or can give me pointers on where I might find a cool resource for this I would be greatly appreciated. \n\nhttps://en.m.wikipedia.org/wiki/Lattice_(order)\n\nhttps://en.m.wikipedia.org/wiki/Partially_ordered_set\n",
      "author": "Master_dreams",
      "created_utc": 1744390134,
      "upvotes": 13,
      "upvote_ratio": 0.93,
      "num_comments": 7,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jwujre/best_data_structure_for_representing_a_partially/"
    },
    {
      "id": "1jvx0g1",
      "title": "Low level programming as in actually doing it in binary lol",
      "content": "I am not that much of a masochist so am doing it in assembly… anyone tried this bad boy?\n\nhttps://www.ebay.com/itm/276666290370",
      "author": "spocek",
      "created_utc": 1744289134,
      "upvotes": 59,
      "upvote_ratio": 0.93,
      "num_comments": 12,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jvx0g1/low_level_programming_as_in_actually_doing_it_in/"
    },
    {
      "id": "1jvrj24",
      "title": "If you had a non-deterministic computer, what would you do with it?",
      "content": "Brainstorming a writing idea and I thought I'd come here. Let's suppose, via supernatural/undefined means, someone is able to create a non-deterministic device that can be used for computation. Let's say it can take a function that accepts a number (of arbitrary size/precision) and return the first positive value for which that function returns true  (or return -1 if no such value exists). Suppose it runs in time equal to the the runtime of the worst case input (or maybe the run time of the first accepted output). Feel free to provide a better definition if you think of one or don't think mine works.\n\nWhat (preferably non-obvious) problems would you try to solve with this?",
      "author": "stgabe",
      "created_utc": 1744267330,
      "upvotes": 59,
      "upvote_ratio": 0.87,
      "num_comments": 74,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jvrj24/if_you_had_a_nondeterministic_computer_what_would/"
    },
    {
      "id": "1jvdxdi",
      "title": "I have come up with an algorithm doing set based topological sort.",
      "content": "It performs topological sort on a directed acyclic graph, producing a linear sequence of sets of nodes in topological order.  The algorithm reveals structural parallelism in the graph.  Each set contains mutually independent nodes that can be used for parallel processing.\n\nI've just finished the [algorithm] (https://github.com/williamw520/toposort/blob/master/Algorithm.md) write-up.\n\n[Implementation](https://github.com/williamw520/toposort) was done in Zig, as I wanted to learn about Zig and it was an opportunity to do a deep dive.",
      "author": "ww520",
      "created_utc": 1744225555,
      "upvotes": 26,
      "upvote_ratio": 0.93,
      "num_comments": 10,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jvdxdi/i_have_come_up_with_an_algorithm_doing_set_based/"
    },
    {
      "id": "1jviwhx",
      "title": "Cannot grasp some concepts from Charles Petzold’s Code",
      "content": "Hey everybody, I've been reading Charles Petzold's book \"Code: The Hidden Language of Computer Hardware and Software\" 2nd edition and seemingly understood everything more or less. I'm now reading the chapter about memory and I can't seem to figure out some things:\n\n1. There's this overview of how to build a 16x8 memory array efficiently. I can understand everything up to the second screenshot. It might be the wording or I stopped following Charles' train of thought at some point. My current understanding is this: the 4 to 16 decoder is used to generate a write signal for a concrete byte. Once generated, all data in values are stored within flip-flops (1st screenshot). Further, however, the author says that those end gates from the decoder are inputs to another set of end gates with another write signal. This is where I'm lost. What is that second write signal? Where does it come from? What's the point of it if the signal generated from the 4 to 16 decoder is seemingly enough to do that 0-1 clock transition and save the value in the flip-flop:\n\n*Processing img wunmckic5gte1...*\n\n*Processing img hlgdjr4k5gte1...*\n\n2. Going further into the chapter, the author shows how we can read the value of a memory cell (the bits at a specific position in each byte are connected in columns). Then he says something I cannot understand, quote: \"At any time, only one of the 16 outputs of the 4-to-16 decoder will have an output of 1, which in reality is a voltage. **The rest will have an output of 0, indicating ground**\". I understand why 1 is voltage but why on earth does he refer to 0 as the ground? From what I understood having read this book for a long time is that the ground is basically a physical connection to the ground (earth) so that the circuit is closed without being visibly closed. Now he refers to the output of 0 as the ground and I'm completely confused. We cannot connect anything there to close the circuit, can we?\n\n*Processing img i8efa2nd6gte1...*\n\n3. And the last but not least, a little further the author says this: \"We could get rid of the giant OR gate if we could just connect all the outputs of the AND gates together. **But in general, directly connecting outputs of logic gates is not allowed because voltages might be connected directly to grounds, and that’s a short circuit.** But there is a way to do this using a transistor, like this:\"\n\n*Processing img hb36678i7gte1...*\n\nAnd again I can't figure out where the ground is in that case and how connecting outputs of logic gates can cause short circuiting. Moreover, he also says this \"If the signal from the 4-to-16 decoder is 1, then the Data Out signal from the transistor emitter will be the same as the DO (Data Out) signal from the memory cell—either a voltage or a ground. **But if the signal from the 4-to-16 decoder is 0, then the transistor doesn’t let anything pass through, and the Data Out signal from the transistor emitter will be nothing—neither a voltage nor a ground.**\". What does this mean? How is nothing different from 0 if, from what I understood, 0 means no voltage and nothing basically also means no voltage?",
      "author": "lesyeuxnoirz",
      "created_utc": 1744238318,
      "upvotes": 7,
      "upvote_ratio": 1.0,
      "num_comments": 7,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jviwhx/cannot_grasp_some_concepts_from_charles_petzolds/"
    },
    {
      "id": "1jv3db7",
      "title": "How do RAM and CPU work together?",
      "content": "I want to understand better the concept of threads and functionality of RAM so please correct me if I am wrong. \n\nWhen u open an app the data, code and everything of that app gets stored in the ram to accessed quickly from there the threads in the cpu cores load up the data from the RAM which then then gets executed by the core and sent back to be displayed.",
      "author": "Fantastic_Kale_3277",
      "created_utc": 1744197763,
      "upvotes": 31,
      "upvote_ratio": 0.88,
      "num_comments": 22,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jv3db7/how_do_ram_and_cpu_work_together/"
    },
    {
      "id": "1jv1l1l",
      "title": "A lot of algorithms in computer science or equations from maths are derived from physics or some other field of science.",
      "content": "Many computer science algorithms or equations in math are derived from physics or some other field of science. The fact that something completely unrelated to the inspiration can lead to something so applicable is, first of all, cool asf.\n\nI've heard about some math equations like the **brachistochrone curve**, which is the shortest path an object under gravity takes to go from one altitude to a lower one—it was derived by **Bernoulli using Snell's law**. Or how a few algorithms in **distributed computing** take inspiration from **Einstein's theory of relativity** (saw this in a video featuring **Leslie Lamport**).\n\nOf course, there's the obvious one—**neural networks**, inspired by the structure of the brain. And from **chemistry**, we’ve got **simulated annealing** used for solving combinatorial optimization problems.\n\nI guess what fascinates me the most is that these connections often weren’t even intentional—someone just noticed a pattern or behaviour in one domain that mapped beautifully onto a completely different problem. The creativity involved in making those leaps is... honestly, the only word that comes to mind is *cool*.\n\nSo here's a question for the community:  \n**What are some other examples of computer science or math being inspired by concepts from physics, chemistry, biology, or any other field?**\n\nWould love to hear some more of these cross-disciplinary connections.\n\n\n**EDIT:** confused on the down votes (⁠ﾉﾟ⁠0ﾟ⁠)⁠ﾉ",
      "author": "AstronautInTheLotion",
      "created_utc": 1744190726,
      "upvotes": 5,
      "upvote_ratio": 0.56,
      "num_comments": 18,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jv1l1l/a_lot_of_algorithms_in_computer_science_or/"
    },
    {
      "id": "1juf5zj",
      "title": "How (or do) game physics engines account for accumulated error?",
      "content": "I've been playing around with making my own simple physics simulation (mainly to implement a force-directed graph drawing algorithm, so that I can create nicely placed tikz graphs. Also because it's fun). One thing that I've noticed is that accumulated error grows rather quickly. I was wondering if this ever comes up in non-scientific physics engines? Or is this ignored? ",
      "author": "JewishKilt",
      "created_utc": 1744123168,
      "upvotes": 123,
      "upvote_ratio": 0.96,
      "num_comments": 48,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1juf5zj/how_or_do_game_physics_engines_account_for/"
    },
    {
      "id": "1ju1lqo",
      "title": "Stanford CS229 - Machine Learning Lecture Notes (+ Cheat Sheet)",
      "content": "Compiled the lecture notes from the Machine Learning course (CS229) taught at Stanford, along with the coinciding \"cheat sheet\".\n\n* [Stanford CS229 - Machine Learning Lecture Notes (+ Cheat Sheet)](https://macro.com/app/pdf/0250fdcf-f9a8-4dac-aa47-d41b6b1542f6/pdf/bcbbb8e4-ab99-4ac2-a149-01690e7994a5)\n\nHere is the YouTube playlist containing the recorded lectures to the course, published by Stanford (Andrew Ng):\n\n* [Stanford CS229: Machine Learning (Andrew Ng)](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)",
      "author": "jstnhkm",
      "created_utc": 1744074714,
      "upvotes": 39,
      "upvote_ratio": 0.97,
      "num_comments": 4,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ju1lqo/stanford_cs229_machine_learning_lecture_notes/"
    },
    {
      "id": "1jtgdi7",
      "title": "How important is Linear Algebra?",
      "content": "Ik it has applications in data analytics, neural networks and machine learning. It is hard, and I actually have learnt it before in uni but I couldn't see the real life applications and now I forgot everything 🤦🏻‍♂️",
      "author": "FirefighterLive3520",
      "created_utc": 1744014474,
      "upvotes": 91,
      "upvote_ratio": 0.9,
      "num_comments": 47,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jtgdi7/how_important_is_linear_algebra/"
    },
    {
      "id": "1js796v",
      "title": "Why do computers always have a single dimensional arrays. Why is memory single-dimensional? Are there any alternatives?",
      "content": "I feel this is to generalize so any kind of N dimensional space can be fit into the same one dimensional memory. but is there more to it?? Or is it just a design choice?",
      "author": "Desperate-Gift7297",
      "created_utc": 1743870324,
      "upvotes": 290,
      "upvote_ratio": 0.94,
      "num_comments": 90,
      "flair": null,
      "url": "https://i.redd.it/i71l2y6ql1te1.png",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1js796v/why_do_computers_always_have_a_single_dimensional/"
    },
    {
      "id": "1jrgesk",
      "title": "How Well Does Bucketsort Work?",
      "content": "Just to let you all know, my job is not in computer science, I am just someone who was curious after browsing Wikipedia. A sort takes an array or linked list and outputs a permutation of the same items but in order.\n\nBubble sort goes through the list, checks if one element is in order of the next one, and then swaps if they are out of order and repeats this until the array is in order.\n\nSelection sort searches for the first element in the list, swaps it so that it occupies the first position, then looks for the second element, swaps it to the second position, looks for the third element, swaps it to the third position, and so on.\n\nInsertion sort I don't really know how to explain well. But it seems to be \"growing\" a sorted list by inserting elements. If the next element is larger than the end of the list you are inserting, you add it to the end, if not, keep swapping until it ends up in the right place. So one side has an already sorted list as the sort is fed unsorted items, It is useful for nearly sorted lists. So I guess if you have a list of 10 million items and you know at most 3,000 are not in their right place, this is great since less than 1/1000 items are out of place.\n\nStooge sort is a \"joke impractical\" sort that made me laugh. I wonder if you can make a sort with an average case of N\\^K with K being whatever integer above 2 you want but a best case of O(N).\n\nQuicksort is kind of a divide and conquer. Pick a pivot point, then put everything below the pivot on one side and everything else on the other side, then do it again on each sublist I guess this is great parallel processing, but apparently this is better than Insertion sort even with serial processing.\n\nBucket sort puts items in buckets and then does a \"real sort\" within each bucket. So I guess you could have a 0 to 1000 bucket, a 1001 to 2000, a 2001 to 3000 and a above 3001 for 4 buckets. This would be very bad if we had 999 items below 1000 and each other bucket had 1 item in it.\n\nAssuming some uniformity in data, how well does Bucket sort compare to quicksort? Say we had 130 buckets, and we were reasonably sure there would be an average of 10 items, we'll say are integers, in each Bucket 3 at a minimum. I'm not even sure how we choose our bucket size. If we commit to 130 buckets and knew our largest integer was 130,000, then each bucket can be 1,000 size. But if you tell your program \"here is a list, sort them into 130 buckets, then do a comparison sort on each bucket\" it would need to find the largest integer. To do that, it would have to go through the entire list. And if it needed to find the largest integer, it could have just done quicksort and start sorting the list without spending time to find the largest one.",
      "author": "ShadowGuyinRealLife",
      "created_utc": 1743784847,
      "upvotes": 20,
      "upvote_ratio": 0.84,
      "num_comments": 23,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jrgesk/how_well_does_bucketsort_work/"
    },
    {
      "id": "1jqgyzj",
      "title": "Was there ever a time where it was widespread to erroneously use kibibyte to mean 1000bytes?",
      "content": "I'm a bit flabbergasted right now and this is genuinely embarrassing. I have a software engineering masters degree from a top university that I graduated from about 20 years ago - and while my memory is admittedly shit, I could have sworn that we learned a kilobyte to be 1024 bytes and a kibibyte to mean 1000bytes - and now I see it's actually the other way around?\nIs my brain just this fucked or was there a time where these two terms were applied the other way around?",
      "author": "MarinatedPickachu",
      "created_utc": 1743682880,
      "upvotes": 142,
      "upvote_ratio": 0.88,
      "num_comments": 173,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jqgyzj/was_there_ever_a_time_where_it_was_widespread_to/"
    },
    {
      "id": "1jpydd0",
      "title": "co-nondeterministic Turing Machines",
      "content": "https://preview.redd.it/dk2tah0j9hse1.png?width=3594&format=png&auto=webp&s=7eaf502ea0b321b0249cc2c7ea2cdeedf354b5e5\n\nHello,   \n  \nso I have an exam coming up and this was one of the question from a previous exam.  \n  \nA simple Turing Machine which we could quickly realize what L\\_N in this case is: { w | w ∈ {a, b}\\* and |w| >= 2 }.  But when it comes to L\\_coN, the language where M behaves as a co-nondeterministic TM, what would the language be? Sure I understand that a coNTM must evaluate every path it takes to true (it accepts) otherwise it would reject, but what does it exactly mean in this context?\n\nAnd for some reason there is no information about such TMs on the the internet, any help would be greatly appreciated! \n\nThank you.",
      "author": "chrysobooga",
      "created_utc": 1743624344,
      "upvotes": 6,
      "upvote_ratio": 0.88,
      "num_comments": 8,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jpydd0/conondeterministic_turing_machines/"
    },
    {
      "id": "1jnz0aq",
      "title": "[blog] if you want to browse the internet, you must first invent the universe",
      "content": null,
      "author": "ashutoshbsathe",
      "created_utc": 1743412401,
      "upvotes": 6,
      "upvote_ratio": 0.64,
      "num_comments": 2,
      "flair": null,
      "url": "https://ashutoshbsathe.github.io/blog/internet-universe/",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1jnz0aq/blog_if_you_want_to_browse_the_internet_you_must/"
    },
    {
      "id": "1jngz27",
      "title": "What exactly is a \"buffer\"",
      "content": "I had some very simple C code:\n\n```clang\nint main() {\n  while (1) {\n    prompt_choice();\n  }\n}\n\nvoid prompt_choice() {\n  printf(\"Enter your choice: \");\n  int choice;\n  scanf(\"%d\", &choice);\n  switch (choice) {\n    case 1:\n      /* create_binary_file(); */\n      printf(\"your choice %d\", choice);\n      break;\n    default:\n      printf(\"Invalid choice. Please try again.\\n\");\n  }\n}\n```\n\nI was playing around with different inputs, and tried out `A` instead of some valid inputs and I found my program infinite looping. When I input `A`, the buffer for `scanf` doesn't clear and so that's why we keep hitting the default condition.\n\nSo I understand to some extent why this is infinite looping, but what I don't really understand is this concept of a \"buffer\". It's referenced a lot more in low-level programming than in higher level languges (e.g., Ruby). So from a computer science perspective, what is a buffer? How can I build a mental model around them, and what are their limitations?",
      "author": "DennisTheMenace780",
      "created_utc": 1743354061,
      "upvotes": 72,
      "upvote_ratio": 0.92,
      "num_comments": 26,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jngz27/what_exactly_is_a_buffer/"
    },
    {
      "id": "1jmqb1l",
      "title": "Inside arXiv—the Most Transformative Platform in All of Science",
      "content": "Really cool article about the people behind something we all take for granted.",
      "author": "Choice-Flower6880",
      "created_utc": 1743266292,
      "upvotes": 50,
      "upvote_ratio": 0.95,
      "num_comments": 1,
      "flair": "Article",
      "url": "https://www.wired.com/story/inside-arxiv-most-transformative-code-science/",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1jmqb1l/inside_arxivthe_most_transformative_platform_in/"
    },
    {
      "id": "1jmawcn",
      "title": "Leading research for consensus mechanisms?",
      "content": "What are the current innovations in this area of study? I'm really interested about the \"cutting edge\" of this, if there's anything like that going on. I feel like a greater emphasis on the efficiency of cryptographic mining will be happening sooner than later, and consensus algorithms will become a prime means of reducing resource use. Any references/dissertations/articles would be appreciated!",
      "author": "Ball-O-Interesting",
      "created_utc": 1743210695,
      "upvotes": 5,
      "upvote_ratio": 0.73,
      "num_comments": 3,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jmawcn/leading_research_for_consensus_mechanisms/"
    },
    {
      "id": "1jlrumb",
      "title": "ask network guys, why upload speed tends to be much slower than download speed?",
      "content": "here, \"speed\" refers to casual, daily-life meaning.\n\nan example is when we upload/download a file(s) to/from a cloud storage service. speed gap is obvious.\n\nI'm not sure but I suspect that one of the reasons is that the server performs safety check on files which will be uploaded on. And this might be enough, but I wonder if there are further reasons.",
      "author": "Gloomy-Status-9258",
      "created_utc": 1743157254,
      "upvotes": 51,
      "upvote_ratio": 0.87,
      "num_comments": 22,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jlrumb/ask_network_guys_why_upload_speed_tends_to_be/"
    },
    {
      "id": "1jlnno8",
      "title": "How do I make programs that are more friendly to the system in terms of performance? Is it worth even trying?",
      "content": "This isn’t a question about algorithmic optimization. I’m curious about how in a modern practical system with an operating system, can I structure my code to simply execute faster. I’m familiar with some low level concepts that tie into performance such as caching, scheduling, paging/swapping, etc. . I understand the impact these have on performance, but are there ways I can leverage them to make my software faster? I hear a lot about programs being “cache friendly.” Does this just mean maintaining a relatively small memory footprint and accessing close by memory chunks more often? Does having immutable data effect this by causing fewer cache invalidations? Are there ways of spacing out CPU and IO bound operations in such a way as to be more beneficial for my process in the eyes of the scheduler? In practice, if these are possible, how would you actually accomplish this in code? Another question I think it worth the discussion, the people who made the operating system are probably much smarter than me. It’s likely that they know better. Should I just stay out of the way and not try to interfere? Would my programs be better off just behaving like any other average program so it can be more predictable? (E to add: I would think this applies to compiler optimizations as well. Where is it worth drawing the line of letting the optimizations do their thing? By going overboard w hand written optimizations, could I be creating less common patterns that the compiler may not be made to optimize as well?) I would assume most discussion around this would also apply mostly to lower level languages like C which I’m fine with. Most code I write these days is C and Rust with some Python for work. \n\nIf you’re curious, I’m particularly interested in this topic for a personal project to develop a solver for [nonagrams](https://en.m.wikipedia.org/wiki/Nonogram). I’m using this as a personal challenge to learn about optimization at all levels. I really want to just push the limits of my skills and optimization. My current, somewhat basic, implementation is written in rust, but I’m planning on rewriting parts in C as I go. ",
      "author": "gman1230321",
      "created_utc": 1743138623,
      "upvotes": 15,
      "upvote_ratio": 0.8,
      "num_comments": 16,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jlnno8/how_do_i_make_programs_that_are_more_friendly_to/"
    },
    {
      "id": "1jlk2d7",
      "title": "not exactly sure if this fits here, but in this building game i like i made a very basic binary computer :D (im not good at computer science i plan to go into the medical field)",
      "content": "basically that REPEATER gate is always active which triggers one part of the AND gate, which that gate's other input is a lever. that triggers an actual repeating REPEATER goes into a DELAY which turns on the binary value \"1,\" and that also triggers an INVERTER, so when that DELAY is off the INVERTER triggers the \"0\" light. do yall think i did good? first time doing anything like this ",
      "author": "failuredude1",
      "created_utc": 1743126699,
      "upvotes": 27,
      "upvote_ratio": 0.83,
      "num_comments": 6,
      "flair": "Discussion",
      "url": "https://i.redd.it/60cd8d4a6cre1.png",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1jlk2d7/not_exactly_sure_if_this_fits_here_but_in_this/"
    },
    {
      "id": "1jkir4v",
      "title": "What are some papers/ thesus/ books every programmer should read",
      "content": null,
      "author": "Stock_Opening_6040",
      "created_utc": 1743013409,
      "upvotes": 107,
      "upvote_ratio": 0.97,
      "num_comments": 46,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jkir4v/what_are_some_papers_thesus_books_every/"
    },
    {
      "id": "1jj5dod",
      "title": "What are active areas of TCS that are not ML-related?",
      "content": "It feels like often when I see a talk at a theory seminar or read a prof's research interests, it is often something along the lines of \"My research lies at the intersection between theoretical computer science and machine learning.\" My question is what are the most active parts of TCS that are not at the intersection with ML?",
      "author": "Tim70",
      "created_utc": 1742858924,
      "upvotes": 35,
      "upvote_ratio": 0.97,
      "num_comments": 11,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jj5dod/what_are_active_areas_of_tcs_that_are_not/"
    },
    {
      "id": "1jiyqpp",
      "title": "Difference between throughput and transfer rate",
      "content": "What is the difference between throughput and transfer rate in terms of sending a file over a network? I’m a bit confused as the terms seem to be the same to me lol. I need to do some experiments where I measure each of them but I’m struggling with what I’m actually measuring for each of them.",
      "author": "MoneyCalligrapher630",
      "created_utc": 1742842807,
      "upvotes": 5,
      "upvote_ratio": 1.0,
      "num_comments": 3,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jiyqpp/difference_between_throughput_and_transfer_rate/"
    },
    {
      "id": "1ji5oea",
      "title": "Help Please! Quadtrees Complexity!",
      "content": "Hello!\n\nI am working on terrain and, long story short, the method I am trying to follow to split it up in levels of details involves quadtrees. I did some digging into the complexity of classic operations (such as adding/removing/retrieving information) with quadtrees and it would seem that it is generally logarithmic. I wanted to find a somewhat detailed proof to understand how the sources I found get to that result, but I couldn't (there also seems to be slightly varying information between it being O(lnN) or O(log2(N)).\n\nWhen I try to figure out the proof on my own (I never really studied CS, so complexity demonstrations are far from my forte) I seem to find something closer to linear complexity (which, if I've understood correctly, would kind of defeat the purpose of using a quadtree since it's the same complexity as a list). One of my proof attempts resulted in constant complexity so I'm obviously making mistakes.\n\nI know this might be asking a lot, but could someone walk me through it please?\n\nThanks in advance!\n\nPS: apologies if I misused math/CS terms, English isn't my first language",
      "author": "Utopia_No1447",
      "created_utc": 1742753591,
      "upvotes": 7,
      "upvote_ratio": 0.9,
      "num_comments": 8,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ji5oea/help_please_quadtrees_complexity/"
    },
    {
      "id": "1jhx2c6",
      "title": "Book for Parallel computing",
      "content": "I feel like I really need a book for parallel computing course. Is there any recommendation simply explained parallel computing?",
      "author": "EnergyParticular2459",
      "created_utc": 1742728910,
      "upvotes": 11,
      "upvote_ratio": 1.0,
      "num_comments": 5,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jhx2c6/book_for_parallel_computing/"
    },
    {
      "id": "1jh8afp",
      "title": "City walking algorithm",
      "content": "**NOTE: This is not a homework assignment, rather a business problem we are trying to solve for a maintenance contract in the downtown core of a major city.** \n\nGiven a square grid/map of the downtown of a modern city, what is the most efficient way to walk the entire surface area of the sidewalks (two on each street, north/south and east/west) with the least amount of overlap and in the least amount of time?\n\nAssumptions:\n\n* a constant speed is assumed\n* 4 people are available to do the walking",
      "author": "theron-",
      "created_utc": 1742649646,
      "upvotes": 72,
      "upvote_ratio": 0.97,
      "num_comments": 26,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jh8afp/city_walking_algorithm/"
    },
    {
      "id": "1jggpq7",
      "title": "How do I get started with writing an research paper & find people to collaborate with?",
      "content": "Hey guys,\nI want to write an ML research paper but have no idea where to start. I’ve worked on deep learning stuff and done NLP projects like sentiment analysis,implementing research papers, fine tuning etc  but never written a proper paper before.How do I get started?\nWhere do people usually find collaborators or Mentors for this?\nIf anyone has experience with this or wants to team up, hit me up! Would love to get some guidance.",
      "author": "Different-Activity-4",
      "created_utc": 1742563399,
      "upvotes": 20,
      "upvote_ratio": 0.85,
      "num_comments": 19,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jggpq7/how_do_i_get_started_with_writing_an_research/"
    },
    {
      "id": "1jfv1tk",
      "title": "New prime algorithm I just made",
      "content": "Hi, I just published a research paper about a new prime generation algorithm that's alot more memory efficient than the sieve of Eratosthenes, and is faster at bigger numbers from some tests I made. Here's the link to the paper : https://doi.org/10.5281/zenodo.15055003 there's also a github link with the open-source python code, what do you think?",
      "author": "Zizosk",
      "created_utc": 1742493389,
      "upvotes": 101,
      "upvote_ratio": 0.78,
      "num_comments": 84,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jfv1tk/new_prime_algorithm_i_just_made/"
    },
    {
      "id": "1jfjirz",
      "title": "Is this a mistake in this textbook?",
      "content": "This example looks more like n^2 than n log n\n\nFoundations of computer science - Behrouz Forouzan",
      "author": "Tall-Wallaby-8551",
      "created_utc": 1742456093,
      "upvotes": 79,
      "upvote_ratio": 0.92,
      "num_comments": 37,
      "flair": "Advice",
      "url": "https://www.reddit.com/gallery/1jfjirz",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1jfjirz/is_this_a_mistake_in_this_textbook/"
    },
    {
      "id": "1jfh8pj",
      "title": "funny thought",
      "content": "I downloaded wireshark today(night) for a networking and security assignment I have due soon and im finally seeing what my internet does. anyone else find themselves wondering just how many of these captured 'wires' are malware packets sending back information to their creator because you downloaded a certain modded mobile app game on a sketchy sight over a year ago",
      "author": "Huge-Wrap-4657",
      "created_utc": 1742446224,
      "upvotes": 12,
      "upvote_ratio": 0.77,
      "num_comments": 2,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jfh8pj/funny_thought/"
    },
    {
      "id": "1jewm2c",
      "title": "How does Wires Computing effect your daily use?",
      "content": "I'm writing an essay for a class and need some users input. The premise is about how Wires effect users and their computing. As in the more we use our devices, such as cell phones, computers, tablets etc. the more we desire everything to be wireless. So when we get a computer that has less ports for example and everything is wireless, such as bluetooth, wifi, wireless hdmi. Does that make the experience better because we need less to do what we want? Or does it make it worse because we feel less in control of the device we're using because we can't simply plug what we need into the unit for it to work?\n\n>Think hdmi for example, you want to hook something to your TV, and hdmi cable is great and a simple solution, we're 100% in control. Most devices have wireless casting built-in now, which can work, but we have to ensure we're on the same network, all the settings are proper etc.\n\nEach has it's pros and cons, have we gotten to the point where we just deal with things, or do we still seek out computers (laptops, tablets) that have more to give us control\n\nSo as in the first question... How do your wires effect your computing?\n\n*\\*\\*Meant to title it \"How do your wires effect your computing?\"*",
      "author": "evilp8ntballer7",
      "created_utc": 1742389855,
      "upvotes": 115,
      "upvote_ratio": 0.87,
      "num_comments": 31,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jewm2c/how_does_wires_computing_effect_your_daily_use/"
    },
    {
      "id": "1jeeb97",
      "title": "Why do games use udp instead of tcp?",
      "content": "I’m learning computer networks right now in school and i’ve learned online games use udp instead of tcp but i don’t really understand why? I understand udp transmits packets faster which I can see being valuable in online games that are constantly updating, but no congestion or flow control or rdt seems like too big of a drawback in them too. Wouldn’t it be better to ensure every packet is accurate in competitive games for accuracy or is udp that much faster that it doesn’t matter? Also, would congestion and flow control help when servers experience a lot of traffic and help prevent lagging and crashing or would it just make it worse?",
      "author": "jaredsowner",
      "created_utc": 1742328013,
      "upvotes": 234,
      "upvote_ratio": 0.96,
      "num_comments": 66,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jeeb97/why_do_games_use_udp_instead_of_tcp/"
    },
    {
      "id": "1jenl3g",
      "title": "How would a Pentium 4 computer perform with today's fabrication technology?",
      "content": "The [Pentium 4](https://en.wikipedia.org/wiki/Pentium_4) processor was launched in 2000, and is one of the last mainstream 32-bit architectures to feature a single core.  It was fabricated using a 130 nm process, and one of the models had a 217 mm^2 die size.  The frequency varied up to 3.8 Ghz, and it could do 12 GFLOP/s.\n\nNowadays though, we can make chips on a 2 nm process, so it stands to reason that we could do a massive die shrink and get a teeny tiny pentium 4 with much better specs.  I know that the process scale is more complicated than it looks, and a 50 nm chip isn't necessarily a quarter of the size of a die-shrunk 100 nm chip. _But_, if it did work like that, a 2 nm die shrink would be 0.05 mm^2 instead of 217.  You could fit over 4200 copies on the original die.  GPU's do something similar, suggesting that one could have a gpu where each shader core has the power of a full-fledged pentium 4.  Maybe they already do? 12 GFlops times 4200 cores suggests a 50 TFlop chip.  Contrast this with the 104 TFlops of a RTX 5090, which is triple the die size, and it looks competitive.  OTOH, the 5090 uses a 5nm process, not 2; so the 5090 still ends up having 67% more flops per mm even after adjusting for density.  But from what I understand, their cores are much simpler, share L1/2, and they aren't going to provide the bells and whistles of a full CPU, including hundreds of instructions, pipelining, extra registers, stacks, etc.\n\nBut back to the 'Pentium 4 nano'.  So you'd end up with a die that's maybe 64 mm^2, and somewhere in the middle is a tiny 0.2x0.2 mm copy of the pentium 4 processor.  Most of the chip is dedicated to interlinks and bond wire, since you need to get the IO fed to a 478 pin package.  If the interlinks are around the perimeter of the CPU itself, they'd have to be spaced about 2 micrometers apart.  The tiny chip would make a negligible amount of heat and take tiny amounts of energy to run.  It wouldn't even need a cpu cooler anymore, as it could be passively cooled due to how big any practical die would be compared to the chip image.  Instead of using 100 watts, it ought to need on the order of 20 milliwatts instead, which is like 0.25% of an led.  There's losses and inefficiencies, things that have a minimal current to activate and stuff, but the point is that the CPU would go from half of the energy use of the system to something akin to a random pull-up resistor.\n\nSo far I'm assuming the new system is still running at the 3.8 Ghz peak.  But since it isn't generating much heat anymore (the main bottleneck), it could be overclocked dramatically.  You aren't going to get multiple terahertz or anything, but considering that the overclock record is [7.1 Ghz](https://forums.digitalspy.com/discussion/269075/pentium-4-overclocked-to-7-1-ghz-ouch), mostly limited by thermals, it should be easy to beat.  Maybe 12 Ghz out of the box without special considerations.  But with the heat problem being solved, you run into other issues like the speed of light.  At 12 ghz, a signal can only move about 9 inches per cycle.  So the ram needs to be less than four inches away for some instructions, round-trip times to the north/south bridge becomes an issue, response times from the bus/ram and peripheral components, there's latency problems like hysteresis from having to dis/charge the mass of a connection wire to transmit a signal, and probably a bunch of other stuff I haven't thought of.\n\n  A workaround is to move components from the motherboard onto the same chip as the CPU.  Intel et al did this a decade ago when they eliminated the north bridge, and they moved the gpu onto the die for mobile (also allowing it to act as a co-processor for video and stuff).  There's also the added bonus of not needing the 471 pin cpu socket, and just running the traces directly to their destinations.  It seems plausible to make a chip that has our nano Pentium 4 on it, the maximum 4 Gb of ram, north bridge, GeForce 4 graphics card, AGP bus, and maybe some other auxiliary components all onto a single little chip.  Perhaps even emulate an 80Gb harddrive off in the corner somewhere.  By getting as much of the hardware onto a single chip as possible, the round-trip distance plummets by an order of magnitude or two allowing for at least 50-200 Ghz clock speeds.  multiple Terahertz is still out [due to Heisenberg](https://www.electropages.com/blog/2022/04/scientists-determine-maximum-theoretical-limit-optoelectronic-devices), but you could still make an early-2000's style desktop computer at least 50 times faster than what was, using period hardware designs.  And the whole motherboard would be smaller than a credit card.\n\nWell, that's my 15 year old idea, any thoughts?  I'm uncertain about the peak performance, particularly things like how hard it would be to generate a clean clock signal at those speeds, or how the original design deals with new race conditions and timing issues.  I also don't know how die shrinks affect TDP, just that smaller means less heat and lower voltages.  Half the surface area might mean half the heat, a quarter, or maybe something weird like T^4 or log.  CD-roms would be a problem (80 pin IDE anyone?), although you could still install windows over a network with the right bios.  The PSU could be much smaller and simpler, and the lower power draw would allow for things like using buck converters instead of large capacitors and other passives.  I'd permit sneaking other new technologies in, just as long as the cpu architecture is constant and the OS can't tell the difference.  Less cooling and wasted space imply that space savings could be had elsewhere, so instead of a big Dell tower, the thing could be a TiTac box with some usb ports and a VGA.  It should be possible to run the video output through usb3 instead of the vga too, but I'm not sure how well AGP would handle it since it predates HDMI by several years.  Maybe just add a vga-usb converter on die to make it a moot point, or maybe they have the same analog pin anyway? P4 was also around the time they were switching to pci express, so while mobos existed with either interface, the AGP comes with extra hurdles with how ram is utilized, and this may cause subtle issues with the overclocking.\n\nThe system on a chip idea isn't new, but the principle could be applied to miniaturize other things like vintage game consoles.  Anything you might add on that could be fun; my old PSP can run playstation and N64 games despite being 30x smaller and including extra hardware like screen, battery, controls, etc.",
      "author": "Ghosttwo",
      "created_utc": 1742353565,
      "upvotes": 27,
      "upvote_ratio": 0.85,
      "num_comments": 14,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jenl3g/how_would_a_pentium_4_computer_perform_with/"
    },
    {
      "id": "1jef1en",
      "title": "Why do IPv4 and IPv6 use constant length addresses?",
      "content": "Why is this preferable to say, an organization that simply has a terminator to the address. (Like null terminated strings.)  \nSuch an organization could be (altho marginally) more efficient, since addresses that take less bytes would be faster and simpler to transmit. It would also effectively never run out of address space. (avoiding the problem we ran into with IPv4- altho yes, I know IPv6 supports an astronomically high number of addresses, so this realistically will never again be a problem.)  \n  \nI ask because I'm developing my own internet system in Minecraft, and this has been deemed preferable in that context. My telecommunications teacher could not answer this, and from his point of view such a system is also preferable. Is there something I'm missing?",
      "author": "Rude-Pangolin8823",
      "created_utc": 1742329754,
      "upvotes": 35,
      "upvote_ratio": 0.91,
      "num_comments": 39,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jef1en/why_do_ipv4_and_ipv6_use_constant_length_addresses/"
    },
    {
      "id": "1jcp5jx",
      "title": "Automata Theory NFA to DFA?",
      "content": "I'm looking at NFA to DFA conversion through subset constriction. In the book I'm reading I believe it shows the {q1,q2} as a DFA state but looking above it I can't see any single transition that leads to both of those states? Can someone explain why it's on there? q2 has not outgoing transitions so I can't see any reason for it to be a DFA state?",
      "author": "Then_Cauliflower5637",
      "created_utc": 1742142101,
      "upvotes": 14,
      "upvote_ratio": 0.82,
      "num_comments": 2,
      "flair": "Help",
      "url": "https://www.reddit.com/gallery/1jcp5jx",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1jcp5jx/automata_theory_nfa_to_dfa/"
    },
    {
      "id": "1jcq1w7",
      "title": "How to define an algorithm for generating a check digit without access to the source code?",
      "content": "I'm stuck on a problem and hoping some of you brilliant minds can offer some guidance. I'm trying to figure out the algorithm used to generate the check digit (the last digit) of a 16-digit ID. I don't have access to the source code or any documentation, so I'm trying to reverse engineer it.\n\nHere's what I know about the **ID structure**:\n\n* XXX-XX-XXXXXXXXXX-Y\n* XXX: Country code.\n* XX: Last two digits of the year (e.g., \"22\", \"23\").\n* XXXXXXXXXX: A 10-digit sequential number, padded with leading zeros.\n* Y: The check digit (0-9).\n\n**Real Examples**: 6432300045512011, 6432300045512028, 6432300045512030, 6432300045512049, 6432300045512053, 6432300045512066\n\n**My Goal**: Determine the algorithm used to calculate Y (the check digit).\n\nWhat I've Tried *(and Why it Failed)*:\n\nI have a dataset of millions of these IDs. I've approached this from several angles, but I'm hitting a wall:\n\n1. **Statistical Analysis**:\n\n* **Check Digit Distribution**: The check digits (0-9) are roughly evenly distributed. A histogram shows no obvious bias.\n* **Correlation Analysis** (Pearson, Spearman, Kendall): Extremely low correlation (< 0.001) between the check digit and any other individual digit or combination of digits. A heatmap confirms this – virtually no correlation.\n* **Modulo Analysis**: I tested taking the sum of the first 15 digits modulo n (where n ranged from 6 to 12). The remainders were uniformly distributed, especially for moduli 10 and 11. This suggests a modulo operation might be involved, but it's not straightforward.\n* **Regression Analysis**: Linear regression models performed very poorly, indicating a non-linear relationship.\n* **Difference Analysis**: I examined the differences between consecutive IDs and their corresponding check digits. The IDs are mostly sequential (incrementing by 1). However, the change in the check digit is unpredictable, even with a small change in the ID.\n\n**Conclusion from Statistical Analysis**: The algorithm is likely good at \"mixing\" the input. There's no simple linear relationship. The sequential nature of the IDs, combined with the unpredictable check digit changes, is a key observation.\n\n2. **Genetic Algorithm**:\n\n**Approach**: I tried to evolve a set of weights (one for each of the first 15 digits) and a modulus, aiming to minimize the error between the calculated check digit and the actual check digit.\n\n**Result**: The algorithm quickly stagnated, achieving only around 10% accuracy (basically random guessing).\n\n3. **Known Algorithms**:\n\nI tested common checksum algorithms (Luhn, CRC, ISBN, EAN) and hash functions (MD5, SHA-1, SHA-256). None of them matched.\n\n4. **Brute-Force (Simulated Annealing)**:\n\nTried a simulated annealing approach to explore the vast search space of possible weights and operations.\n\n**Result**: Computationally infeasible due to the sheer number of combinations, especially given the strong evidence of non-linearity.\n\n5. **Neural network**\n\n**Architecture**: Simple fully connected network (15 inputs → hidden layers → 1 output).\n\nSince I am not an expert in machine learning, the neural network predictably failed to produce any results. The learning progress stopped quickly and halted at **10% accuracy**, which corresponds to complete randomness.\n\nThe algorithm likely involves non-linear operations before or after the weighted sum (or instead of it entirely). Possibilities include:\n\n* Perhaps bitwise operations (XOR, shifts, etc.) are involved, given the seemingly random nature of the check digit changes.\n* Something more complex than a simple sum % modulus might be happening.\n* Each digit might be transformed by a function (e.g., exponentiation, logarithm, lookup table) before being weighted.\n\n**My Questions for the Community**:\n\n1. Beyond what I've tried, what other techniques could I use to analyze this type of check digit algorithm? I'm particularly interested in methods that can handle non-linear relationships.\n2. Are there any less common checksum or cryptographic algorithms that I should investigate? I'm looking for anything that might produce this kind of \"well-mixed\" output.\n3. Could Neural Networks be a viable approach here? If so, what kind of architecture and training data would be most effective? I'm thinking about using a sequence-to-one model (inputting the first 15 digits, predicting the 16th). What are the potential pitfalls?\n4. Is it make sense to try to find collisions, when two diffrent numbers produce the same control number?\n\nI'm really eager to hear your ideas and suggestions. Thanks in advance for your help!",
      "author": "nameless_yep",
      "created_utc": 1742144456,
      "upvotes": 12,
      "upvote_ratio": 0.78,
      "num_comments": 4,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jcq1w7/how_to_define_an_algorithm_for_generating_a_check/"
    },
    {
      "id": "1jcfwvy",
      "title": "Self-study roadmap for Quantum Computing",
      "content": "Prerequisites:\n- linear algebra (vectors, matrices, eigenvalues, tensor products)\n- complex numbers\n- if you know the basics of quantum mechanics then well done\n- calculus \n- Probability theory (i would recommend it for quantum algorithms & information theory)\n\nBasics:\n1) For interactive intro: https://quantum.country/qcvc\n2) Old is gold yk so go through this playlist:\nhttps://www.youtube.com/watch?v=F_Riqjdh2oM&list=PL1826E60FD05B44E4\n3) For quantum circuit & gates:\nhttps://qiskit.org/textbook/\n4) To run simple simple quantum programs:\nhttps://quantum-computing.ibm.com/\n\nIntermediate:\nWelcome homie\n1) Principles of Quantum Computation and Information - Volume I then II\n2) Quantum algorithms - https://qiskit.org/textbook/ch-algorithms/\n3) For physics part:\nhttps://www.youtube.com/watch?v=w08pSFsAZvE&list=PL0ojjrEqIyPy-1RRD8cTD_lF1hflo89Iu\n4) Practice coding quantum algorithms using Qiskit or Cirq\nhttps://quantumai.google/cirq/tutorials \n\nAdvance level:\nI myself not aware of much here but if you wanna explore research oriented side and theoretical knowledge then i know some books.\n1) Quantum Computation and Quantum Information by Nielsen & Chuang\n2) An Introduction to Quantum Computing by Kaye, Laflamme & Mosca\n3) IBM Quantum Experience and Amazon Braket https://aws.amazon.com/braket/ for cloud-based quantum computing.\n\n\nQuantum computing is vast so learning it in a month or day (humph not possible) you can also learn quantum complexity theory but this is focused on practical quantum computing.",
      "author": "Lexouwuop",
      "created_utc": 1742108766,
      "upvotes": 49,
      "upvote_ratio": 0.96,
      "num_comments": 7,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jcfwvy/selfstudy_roadmap_for_quantum_computing/"
    },
    {
      "id": "1jc4klh",
      "title": "As We May Think (1945)",
      "content": null,
      "author": "breck",
      "created_utc": 1742071092,
      "upvotes": 12,
      "upvote_ratio": 0.93,
      "num_comments": 1,
      "flair": "Article",
      "url": "http://breckyunits.com/asWeMayThink.html?r",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1jc4klh/as_we_may_think_1945/"
    },
    {
      "id": "1jblzw6",
      "title": "How do you create a new programming language?",
      "content": "Hey, inexperienced cs student here. How does one create a new programming language? Don't you need an existing programming language to create a new programming language? How was the first programming language created?",
      "author": "Shoocceth",
      "created_utc": 1742008588,
      "upvotes": 173,
      "upvote_ratio": 0.9,
      "num_comments": 61,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jblzw6/how_do_you_create_a_new_programming_language/"
    },
    {
      "id": "1jbv5rl",
      "title": "Memory bandwidth vs clock speed",
      "content": "I was wondering, \n\nWhat type of process are more subject to take advantage of high memory bandwidth speed (and multi threading) ?\n\nAnd what type of process typically benefits from cores having high clock speed ?\n\nAnd if there is one of them to prioritize in a system, which one would it be and why ?\n\nThanks !",
      "author": "Rim3331",
      "created_utc": 1742045457,
      "upvotes": 6,
      "upvote_ratio": 1.0,
      "num_comments": 5,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jbv5rl/memory_bandwidth_vs_clock_speed/"
    },
    {
      "id": "1jb0upr",
      "title": "I found this book while searching for something related to Algorithms",
      "content": "Hey guys\nI found this book in my closet \nI never knew I had this\nCan this book be useful?\nIt says 3d visualisation \nSo what should I know in order to get to know the contents of this?",
      "author": "StructureOld7019",
      "created_utc": 1741948475,
      "upvotes": 152,
      "upvote_ratio": 0.96,
      "num_comments": 21,
      "flair": "Help",
      "url": "https://i.redd.it/f691nm05vmoe1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1jb0upr/i_found_this_book_while_searching_for_something/"
    },
    {
      "id": "1jb230n",
      "title": "How do companies use GenAI?",
      "content": "I work for a F500 and we are explicitly told not to use GenAI outside of Co-Pilot. It’s been the same at both the places I worked at since genAI “took over”.\n\nTo me, it feels like GenAI is replacing stackoverflow mostly. Or maybe boilerplates at max. I’ve never seen anyone do architectural design using GenAI.\n\nHow do you use GenAI at work? Other than bootstrapped startups, who is using GenAI to code?",
      "author": "Negative-Drawer2513",
      "created_utc": 1741953099,
      "upvotes": 13,
      "upvote_ratio": 0.84,
      "num_comments": 4,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jb230n/how_do_companies_use_genai/"
    },
    {
      "id": "1jaoyu1",
      "title": "Etymology of Cookies.",
      "content": "I was explaining what cookies actually ARE to my roommate. She asked why the name and I was stu.oed. of course Wikipedia has all the I fo on all the different kinds and functions but the origin of the name literally says it is a reference to \"Magic cookies\" sometimes just called Cookies. And the article for *that* doesn't address why tf THOSE were named cookies. \n\n\nAnybody know the background history on this?\n\nUntil I learn some actual facts im just gonna tell people that they are called cookies because magic internet goblins leave crumbs in your computer whenever you visit their websites. ",
      "author": "fredoillu",
      "created_utc": 1741905721,
      "upvotes": 30,
      "upvote_ratio": 0.95,
      "num_comments": 10,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jaoyu1/etymology_of_cookies/"
    },
    {
      "id": "1jaozn4",
      "title": "Graph theory and its application",
      "content": "Graph theory in real world applications\n\nI've been interested lately in graph theory, I found it fun but my issue is that I can't really formulate real world applications into graph theory problems. I would pick a problem X that I might think it can be formulated as a graph problem, If I make the problem X so simple it works but as soon as I add some constraints i can't find a way to represent the problem X as a graph problem that is fundamental in graph theory.. I want to use the fundamental graph theories to resolve real world problems.\nI am no expert on the field so it might be that it's just a skill issue",
      "author": "Snoo-16806",
      "created_utc": 1741905780,
      "upvotes": 27,
      "upvote_ratio": 0.94,
      "num_comments": 31,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jaozn4/graph_theory_and_its_application/"
    },
    {
      "id": "1jafwbw",
      "title": "AMA with Stanford CS professor and co-founder of Code in Place today @ 12pm PT",
      "content": "Hi r/computerscience, Chris Piech, a CS professor at Stanford University and lead of the free Code in Place program here at Stanford is doing an AMA today 12pm PT, and would love to answer your Qs!\n\nHe will be answering Qs about: learning Python, getting starting in programming, how you can join the global Code in Place community, and more.\n\nAMA link: [https://www.reddit.com/r/AMA/comments/1j87jux/im\\_chris\\_piech\\_a\\_stanford\\_cs\\_professor\\_passionate/](https://www.reddit.com/r/AMA/comments/1j87jux/im_chris_piech_a_stanford_cs_professor_passionate/)\n\nThis is the perfect chance to get tips, insights, and guidance directly from someone who teaches programming, and is passionate about making coding more accessible.\n\nDrop your questions or just come learn something new!",
      "author": "Stanford_Online",
      "created_utc": 1741882902,
      "upvotes": 25,
      "upvote_ratio": 1.0,
      "num_comments": 4,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1jafwbw/ama_with_stanford_cs_professor_and_cofounder_of/"
    },
    {
      "id": "1j9kfyz",
      "title": "CS research",
      "content": "Hi guys, just had an open question for anyone working in research - what is it like? What do you do from day to day? What led you to doing research as opposed to going into the industry? \nI’m one of the run of the mill CS grads from a state school who never really considered research as an option, (definitely didn’t think I was smart enough at the time) but as I’ve been working in software development, and feeling, unfulfilled by what I’m doing- that the majority of my options for work consist of creating things or maintaining things that I don’t really care about, I was thinking that maybe I should try to transition to something in research.\nThanks for your time! Any perspective would be awesome.\n",
      "author": "Geohindrix1",
      "created_utc": 1741787179,
      "upvotes": 56,
      "upvote_ratio": 1.0,
      "num_comments": 21,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j9kfyz/cs_research/"
    },
    {
      "id": "1j8q0xv",
      "title": "How does CPU knows how to notify OS when a SysCall happen?",
      "content": "Supposing P1 has an instruction that makes a Syscall to read from storage, for example. In reality, the OS manage this resource, but my doubt is, the program is already in memory and read to be executed by the CPU which will take that operation and send it to the storage controller to perform it, in this case, an i/o operation. Suppose the OS wants to deny the program from accessing the resource it wants, how the OS sits in between the program and CPU to block it if the program is already in CPU and ready to be executed?\n\nhttps://preview.redd.it/l7uei0dj32oe1.png?width=964&format=png&auto=webp&s=6e7cd71c089bdc4f9b5dd756d22f7f47f3ef2c92\n\nI don't know if I was clear in my questioning, please let me know and I will try to explain it better.\n\nAlso,if you did understand it, please be as deep as you can in the subject while answering, I will be very grateful.",
      "author": "Emergency_Status_217",
      "created_utc": 1741697582,
      "upvotes": 38,
      "upvote_ratio": 0.95,
      "num_comments": 23,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j8q0xv/how_does_cpu_knows_how_to_notify_os_when_a/"
    },
    {
      "id": "1j8b06a",
      "title": "How does an IDE work, and really any other program?",
      "content": "I am having trouble articulating this question because my minuscule knowledge of CS, but here goes. How exactly does an IDE work, let’s say that it’s a Java IDE, what language is the IDE created in? And what compiles the IDE software? I’m trying to learn computer science, but I don’t have any teachers, and I feel like I have somewhat of a crumbling foundation and a weak grasp on the whole concept, I want to understand how every little bit makes something tick, but I always end up drowning in confusion, so help would be much appreciated!",
      "author": "Jdwg128",
      "created_utc": 1741644739,
      "upvotes": 123,
      "upvote_ratio": 0.89,
      "num_comments": 62,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j8b06a/how_does_an_ide_work_and_really_any_other_program/"
    },
    {
      "id": "1j8ayg6",
      "title": "How does a “window” work?",
      "content": "How exactly do “screens” go on top of one another on a computer screen, really think about that, how does the computer “remember” all of the pixels that were “under” the bottom window when you close it out, and redisplay them? I’m trying to learn computer science, but I don’t have any teachers, and I feel like I have somewhat of a crumbling foundation and a weak grasp on the whole concept, I want to understand how every little bit makes something tick, but I always end up drowning in confusion, so help would be much appreciated!",
      "author": "Jdwg128",
      "created_utc": 1741644617,
      "upvotes": 58,
      "upvote_ratio": 0.86,
      "num_comments": 11,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j8ayg6/how_does_a_window_work/"
    },
    {
      "id": "1j8kmdp",
      "title": "I'd like to read up on the following topic: (if there is info on it?) When given an unrooted tree, pick a node as the root, what patterns/relationships can be observed in the new tree that is formed compared to picking other nodes as the root?",
      "content": "To elaborate, are there any cool mathematical ideas that are formed? Any real life applications to choosing different roots? Are there any theorems on this? Is this a well researched topic or just a dead end lame idea?  \n\n\nPotential question: Given an unrooted tree with n vertices can you choose a root such that the height of the tree is h where h is any natural number > 0 and <= n? Is there a way to prove it's only possible for some h? I haven't played around with this problem yet.\n\nI feel like there could be some sort of cool game or other weird ideas here. Visually the notion of choosing different roots reminds me of the different shapes you get if you lay a tissue flat on a table and pick it up at different points, so I wouldn't be surprised if there are some sort of topological ideas going on here",
      "author": "BilliDaVini",
      "created_utc": 1741675081,
      "upvotes": 7,
      "upvote_ratio": 0.77,
      "num_comments": 16,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j8kmdp/id_like_to_read_up_on_the_following_topic_if/"
    },
    {
      "id": "1j8c09n",
      "title": "Lambda Calculus",
      "content": "I have taken an interest in lambda calculus recently, however I have ran into an issue. Each textbook or course use different notation, there is Church notation, there is also notation that uses higher order functions and words to describe the process, another notation that I have encountered was purely mathematical I believe, it looked like church notation, but twice as long. It is a pity that while this field of computer science is appealing to me, I struggle to grasp it because of my uncertainty pertaining to which notation I should use. I don't enjoy the use of higher order functions since I want to form a deep understanding of these subjects, however I am not planning on writing page long functions either. Any good resources and advice on which notation I should use is welcome. Also I apologise if my english is not coherent, it is not my first language, if I have made any mistakes that hinder your understanding of my question, feel free to correct me. Thank you in advance :)\n\nTLDR: Confusion about notation in lambda calculus; Displeasement with using higher order functions; Looking for advice on notation type and relevant resources.",
      "author": "[deleted]",
      "created_utc": 1741647274,
      "upvotes": 8,
      "upvote_ratio": 0.75,
      "num_comments": 9,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j8c09n/lambda_calculus/"
    },
    {
      "id": "1j8bonm",
      "title": "Zoltan's FLOPs – GPU mini-grant, 1st iteration",
      "content": null,
      "author": "tczoltan",
      "created_utc": 1741646496,
      "upvotes": 5,
      "upvote_ratio": 0.86,
      "num_comments": 0,
      "flair": null,
      "url": "https://tcz.hu/zoltans-flops",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1j8bonm/zoltans_flops_gpu_minigrant_1st_iteration/"
    },
    {
      "id": "1j7zsvc",
      "title": "Circuit Compiler",
      "content": "Recently I wrote a small compiler\n\nIt job is to take in a truth table e.g:\n\nA B | X\n--\n0 0 | 1\n\n0 1 | 1\n\n1 0 | 0\n\n1 1 | 1 \n\nAnd output a circuit in the form of a Boolean expression, e.g:\n\n((~A)&(~B))|((~A)&(B))|((A)&(B))\n\nI was hoping that some people here would have some feedback on it!\n\nAlso if anyone knows of any events here is the UK that have beginners into compilers then please send a DM!\n\nHere is the code: https://github.com/alienflip/cttube, for anyone interested 🙂\n",
      "author": "AlienFlip",
      "created_utc": 1741616659,
      "upvotes": 11,
      "upvote_ratio": 0.92,
      "num_comments": 2,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j7zsvc/circuit_compiler/"
    },
    {
      "id": "1j7dcam",
      "title": "How to learn gpu architecture?",
      "content": "Hey guys\nCurrently I am learning about computer graphics and graphics api\nTo enhance my knowledge about how graphics api processes things(and on a level of curiosity as well)\nI have decided to learn about the gpu architecture \nBut the issue is I have no clue where to begin with\nAlso I dont know a lot of cpu architecture(If it's essential)\nWhere should I begin?\nAny book of courses(prefered)",
      "author": "Opposite_Squirrel_32",
      "created_utc": 1741543056,
      "upvotes": 16,
      "upvote_ratio": 0.83,
      "num_comments": 11,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j7dcam/how_to_learn_gpu_architecture/"
    },
    {
      "id": "1j6xkmi",
      "title": "What is the differences between Computer Engineering(CE)and Computer Science?(CS)",
      "content": null,
      "author": "[deleted]",
      "created_utc": 1741487439,
      "upvotes": 86,
      "upvote_ratio": 0.91,
      "num_comments": 55,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j6xkmi/what_is_the_differences_between_computer/"
    },
    {
      "id": "1j6jz0h",
      "title": "r1_vlm - an opensource framework for training visual reasoning models with GRPO",
      "content": null,
      "author": "dragseon",
      "created_utc": 1741449057,
      "upvotes": 44,
      "upvote_ratio": 0.96,
      "num_comments": 1,
      "flair": "General",
      "url": "https://i.redd.it/3ssvmck4mhne1.gif",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1j6jz0h/r1_vlm_an_opensource_framework_for_training/"
    },
    {
      "id": "1j6sv69",
      "title": "anyone know where to find network topology art?",
      "content": "Im trying to find art and designers capable of such a thing. Preferrably in motion but any is fine.",
      "author": "Esper_18",
      "created_utc": 1741473174,
      "upvotes": 11,
      "upvote_ratio": 0.76,
      "num_comments": 2,
      "flair": "Advice",
      "url": "https://i.redd.it/47rspriuljne1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1j6sv69/anyone_know_where_to_find_network_topology_art/"
    },
    {
      "id": "1j63vsa",
      "title": "could i create a data packet, set the ttl to one trillion, and then send it across the internet and just have it live forever",
      "content": "like, it would just keep hopping onto different routers forever, and never die",
      "author": "anbehd73",
      "created_utc": 1741391250,
      "upvotes": 137,
      "upvote_ratio": 0.95,
      "num_comments": 32,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j63vsa/could_i_create_a_data_packet_set_the_ttl_to_one/"
    },
    {
      "id": "1j5mi7h",
      "title": "Could i extend my browser to interpret other languages besides Javascript?",
      "content": "How hard would it be to  make my browser (i use firefox) recognize other programming languages? Let's say i have an small lisp like language that does calculations:\n\n`(+ 3 (car '(2 5 1)) 7)`\n\n Would i be able to put an \"<script language=lisp>\" so firefox recognizes that language? \n\n I would imagine that i would need to build an interpreter and do an condition like this =\n\n`If (language == \"lisp\") {`\n\n `useMyInterpreter()`\n\n`} else {`\n\n`useSpiderMonkey()`\n\n`}` \n\n But then, there's also the issue on how to render the result into html. \n\n Any resources on this whole thing?\n\n",
      "author": "Mykhavunish",
      "created_utc": 1741351044,
      "upvotes": 31,
      "upvote_ratio": 0.91,
      "num_comments": 16,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j5mi7h/could_i_extend_my_browser_to_interpret_other/"
    },
    {
      "id": "1j4uih0",
      "title": "A Quick Journey Into the Linux Kernel",
      "content": null,
      "author": "lucavallin",
      "created_utc": 1741264683,
      "upvotes": 125,
      "upvote_ratio": 0.94,
      "num_comments": 13,
      "flair": "Article",
      "url": "https://www.lucavall.in/blog/a-quick-journey-into-the-linux-kernel",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1j4uih0/a_quick_journey_into_the_linux_kernel/"
    },
    {
      "id": "1j52ur8",
      "title": "How/when can I get started with research?",
      "content": "Idk if this is the right sub 😭😭😭\n\nI’m really liking my discrete math course (well proofs / discrete math for CS majors lol) and want to pursue research in TCS. I’m only a freshman (well moreso first-year, I’m a second semester sophomore by credit) and want to get into research, but I don’t know if I’m far enough to get started. I have my calc I + II credit from BC in HS and AP stats, I did linear data structures last semester and I’m doing non-linear data structures + a C praticum this semester, and the discrete math course. Next semester, I’m looking to do algorithms, probability (for CS majors lol), and programming methodology. Am I good to start looking for research now, at the end of this semester, or should I wait until the end of next semester?",
      "author": "ConsideringCS",
      "created_utc": 1741286999,
      "upvotes": 24,
      "upvote_ratio": 0.94,
      "num_comments": 5,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j52ur8/howwhen_can_i_get_started_with_research/"
    },
    {
      "id": "1j41jzf",
      "title": "Are computers pre programmed?",
      "content": "I starte learning python for the first time as a side hustle. I have this question in my mind that\" How computer knows that 3+5 is 8 or when i say ring alarm\". How do computer know what alarm mean?? Is this window who guide or processor store this information like how the hell computers works 😭. ",
      "author": "PRB0324",
      "created_utc": 1741176361,
      "upvotes": 216,
      "upvote_ratio": 0.79,
      "num_comments": 102,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j41jzf/are_computers_pre_programmed/"
    },
    {
      "id": "1j3z4jy",
      "title": "How could a multi tape Turing Machine be equivalent to a single tape when single tape can loop forever?",
      "content": "It seems like the multi tape one has a harder time looping forever than the single tape, because all tapes would have to loop. What am I missing?",
      "author": "Valuable-Glass1106",
      "created_utc": 1741166146,
      "upvotes": 38,
      "upvote_ratio": 0.91,
      "num_comments": 21,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j3z4jy/how_could_a_multi_tape_turing_machine_be/"
    },
    {
      "id": "1j3upnk",
      "title": "Google maps / Uber Routing alogrithm",
      "content": "I'm looking for research papers on the routing algorithms used in Google Maps, Uber, or similar real-time navigation systems. If anyone knows of good academic papers, whitepapers, or authoritative blog posts on these topics, please drop the links or recommendations . ",
      "author": "New-Zookeepergame261",
      "created_utc": 1741148259,
      "upvotes": 18,
      "upvote_ratio": 1.0,
      "num_comments": 11,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j3upnk/google_maps_uber_routing_alogrithm/"
    },
    {
      "id": "1j3emp9",
      "title": "Mistake in CODE by Charles Petzold",
      "content": "“The abbreviation addr refers to a 16-BYTE address given in the 2 bytes following the operation code”\n\nHow can a 16 BYTE address be given in 2 bytes? Surely he means a 16 bit address? Because 2 bytes is 16 bits?",
      "author": "nineinterpretations",
      "created_utc": 1741105473,
      "upvotes": 53,
      "upvote_ratio": 0.81,
      "num_comments": 25,
      "flair": null,
      "url": "https://i.redd.it/v4s14u0h8pme1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1j3emp9/mistake_in_code_by_charles_petzold/"
    },
    {
      "id": "1j36u7q",
      "title": "Why isn't HCI more popular as a subject?",
      "content": "Human-Computer Interaction perfectly fits the idea of most people's motivation to study CS, It's a prospective underrated field, an seems generally enjoyable for the most part.",
      "author": "OhioDeez44",
      "created_utc": 1741079694,
      "upvotes": 182,
      "upvote_ratio": 0.87,
      "num_comments": 98,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j36u7q/why_isnt_hci_more_popular_as_a_subject/"
    },
    {
      "id": "1j2j5t3",
      "title": "How can unlabeled data help in machine learning?",
      "content": "It seems to me that unlabeled data to a computer is meaningless, because it doesn't get any feedback.\n\nEdit: It seems to me that perhaps my question wasn't clear enough. I'm not asking about specific use cases of semi-supervised learning or whatever. I just don't understand in principle how unlabeled data can help the machine \"learn\".",
      "author": "Valuable-Glass1106",
      "created_utc": 1741010084,
      "upvotes": 6,
      "upvote_ratio": 0.72,
      "num_comments": 13,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j2j5t3/how_can_unlabeled_data_help_in_machine_learning/"
    },
    {
      "id": "1j25ua3",
      "title": "What is the purpose of hypervisor drivers?",
      "content": "I’ve seen some videos explaining hypervisors, but couldn’t figure out the purpose of hypervisor drivers that run within the system, like this:\n\nhttps://github.com/wbenny/hvpp",
      "author": "Ambitious_Corner_852",
      "created_utc": 1740961304,
      "upvotes": 26,
      "upvote_ratio": 0.87,
      "num_comments": 4,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j25ua3/what_is_the_purpose_of_hypervisor_drivers/"
    },
    {
      "id": "1j1kuyq",
      "title": "Can computing the value of the Busy Beaver function for a specific input be used to solve the Goldbach Conjecture?",
      "content": "I understand that we can encode the Goldbach Conjecture into a 27-state Turing Machine. I also understand that if we know the value of BB(27) then we can solve the Goldbach Conjecture by running the 27-state machine and checking whether it halts before BB(27) number of steps.\n\nHowever, isn’t the only way to calculate BB(27) by determining whether or not the 27-state Goldbach Conjecture machine halts or not? Even if we managed to prove that every single 27-state Turing Machine except the Goldbach machine halted, we still wouldn’t know if the Goldbach machine halted with a greater number of steps than all the other machines or if it would never halt. The only way we could know that is by proving the Goldbach Conjecture itself!\n\nSo in other words, it seems to me like the Busy Beaver function is useless for solving the Goldbach conjecture, even if we had an arbitrary amount of computing power. The reason I made this post is that in YouTube videos and forum posts I see people surprised that the BB function can be used to brute force the answer to the Goldbach conjecture, yet that’s not true if my reasoning above holds.",
      "author": "Flarzo",
      "created_utc": 1740898104,
      "upvotes": 23,
      "upvote_ratio": 0.81,
      "num_comments": 24,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j1kuyq/can_computing_the_value_of_the_busy_beaver/"
    },
    {
      "id": "1j0z1i1",
      "title": "Build a simple distributed text-editor with position-based CRDTs",
      "content": "Learn so much from this post alone! \n\nhttps://learntocodetogether.com/position-based-crdt-text-editor/\n\nI've been hearing about CRDTs for quite some time, and I never made any serious effort to learn about them. But this time is great when I learn many interesting things together from some mathematical properties to some concrete CRDT implementation. Please correct me if I make any mistake.\n\nIn the past few months, there has been a shift in how I approach things. Before I typically felt that I could only understand something if I could implement this in some programming language. Now I feel this alone is not enough, and for some fundamental concepts it's important to understand them in a formal context, and typically the things I try to learn could be formalized in some math. So now I try to formalize as much as I can, as I tried to do so in this blog post. \n\nAs this turns out I could understand things on a deeper level, and when trying to formalize as much as possible and go to concrete implementation. Because I can miss some details in my concrete implementations if I failed or just have a slight misunderstanding of the underlying principles. Theory matters, this is when the abstract understanding is fueled with the practice. \n\nWhen I try to write something formally, it indeed helps me improve my abstract reasoning, critical thinking, and understanding of things at a greater level! (and you should try this too!)\n\n",
      "author": "vannam0511",
      "created_utc": 1740834361,
      "upvotes": 13,
      "upvote_ratio": 0.77,
      "num_comments": 2,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1j0z1i1/build_a_simple_distributed_texteditor_with/"
    },
    {
      "id": "1izivni",
      "title": "Are theoretical algorithms ever really \"awkward\" to write in code?",
      "content": "I am doing a senior thesis on a theoretical computer science problem. I have all my theoretical results set. However, I'm starting to write simulations for some of the algorithms. Essentially, I'm finding it's a bit \"awkward\" to implement some of my theoretical algorithms precisely. There's this one little thing due to \"breaking ties\" that's I'm kind of finding it hard to implement precisely. \n\nSince it's just synthetic data simulations, I'm just going to kind of \"cheat\" and do a more imprecise workaround.\n\nDoes anyone else ever run into a similar situation?",
      "author": "Anxious_Positive3998",
      "created_utc": 1740671434,
      "upvotes": 136,
      "upvote_ratio": 0.97,
      "num_comments": 22,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1izivni/are_theoretical_algorithms_ever_really_awkward_to/"
    },
    {
      "id": "1izle43",
      "title": "Resource Learning Advice: Hardware",
      "content": "Does anyone have good resources on topics like: Micro-controllers, micro-processors, Firmwares, BIOS, ROM, Flash memory, reverse engineering...\n\nSorry, it's a lot of topics. they are related even though I feel like I can't descibe them as just hardware.\n\nI would like to understand what is happening to the binaries stored in the metal, how are they stored, how are they troubleshooted. How there are non open sources OSs if the binaries are there and one could reverse it. \n\nSo, I feel that in order to understand it I need deeper knowledge.\n\nI have basic knowledge of ARM assembly language, and how OS works in general, but I wanna decrease these abstractions on my mind and understand the underneath better.\n\nIf you have any good resource, course or books, articles, I appreciate.",
      "author": "Emergency_Status_217",
      "created_utc": 1740677601,
      "upvotes": 11,
      "upvote_ratio": 0.92,
      "num_comments": 2,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1izle43/resource_learning_advice_hardware/"
    },
    {
      "id": "1iyxdwx",
      "title": "How do you tell the difference between Turing Machine looping and just running for a long time?",
      "content": "There's a theorem which states equivalence between TM and an Enumerator. Proving Enumerator => TM, we get input \"w\" to a TM and simply check whether Enumerator prints it or not. If \"w\" appears on the list we accept. If Enumerator runs indefinitely then reject by looping. But how can we know that a TM is looping?",
      "author": "Valuable-Glass1106",
      "created_utc": 1740601724,
      "upvotes": 63,
      "upvote_ratio": 0.91,
      "num_comments": 31,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iyxdwx/how_do_you_tell_the_difference_between_turing/"
    },
    {
      "id": "1iyka7m",
      "title": "Understanding Automatic Differentiation and Dual Numbers",
      "content": "Recently I saw [this video](https://www.youtube.com/watch?v=QwFLA5TrviI) from Computerphile about automatic differentiation and dual numbers which piqued my interest. I understand the dual numbers, it's basically an infinitesimal added to some real number that algebraically works similar to complex numbers. Considering that derivatives evaluate infinitesimal step sizes it makes sense why they work. But it is the algorithm part which doesn't quite make sense. Plugging in a dual number into a function evaluates both the function and its derivative at the value of the real component. But that seems like a typical plug & chug instead of an algorithm like finite difference. Can't see where the algorithm part is. I have no idea where to start when trying to analyze its complexity like with other algorithms (unless I assume it is evaluated using Horner's method or something similar which would be O(n)). All I understand is that dual numbers and forward mode automatic differentiation are mathematically equivalent (based on answers from [this post](https://scicomp.stackexchange.com/questions/41471/why-are-dual-numbers-needed-only-in-forward-mode-autodiff)) so by that logic I assume dual numbers are the algorithm. But this seems to me more like a software design choice like OOP than an actual algorithm. Reverse mode automatic differentiation seems more like an algorithm to me since it breaks down the function into smaller parts and evaluates each part using dual numbers, combining the results to form larger parts until the final solution is found. But what is the actual algorithm behind automatic differentiation? How can its complexity be analyzed? \n\nComputerphile: Forward Mode Automatic Differentiation  \n[https://www.youtube.com/watch?v=QwFLA5TrviI](https://www.youtube.com/watch?v=QwFLA5TrviI)",
      "author": "macroxela",
      "created_utc": 1740564695,
      "upvotes": 15,
      "upvote_ratio": 1.0,
      "num_comments": 2,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iyka7m/understanding_automatic_differentiation_and_dual/"
    },
    {
      "id": "1ixmn3m",
      "title": "Donald Knuth and his books",
      "content": "Hi folks, Does anyone here have experience with Donald Knuth’s books? I heard they’re highly recommended. Yes, we have amazon reviews to look at how really his books are but still looking for some more opinions.",
      "author": "Remarkable_Baker342",
      "created_utc": 1740457855,
      "upvotes": 53,
      "upvote_ratio": 0.94,
      "num_comments": 34,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ixmn3m/donald_knuth_and_his_books/"
    },
    {
      "id": "1ix77yw",
      "title": "Kids programming ideas that arent games (already knows scratch)",
      "content": "My 9 year old has been doing scratch for a couple years. She understands it pretty well and loves following projects, but has little interest in being creative and making up games. She started reading thevSecret Coders series and loves it.\n\nWhat can she do to utilize her love of coding/computers, but is more functional than entertaining? Every time I look at coding for kids, it teaches games. She works better with accomplishing a set goal. \n\nEdit: I looked into Arduino from your suggestions. We already have Lego Boost which is similar enough (and can program with scratch). Im starting to think html/javascript might be a good option. Instant feedback and more about visual than logic. ",
      "author": "snoopmt1",
      "created_utc": 1740417194,
      "upvotes": 67,
      "upvote_ratio": 0.91,
      "num_comments": 44,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ix77yw/kids_programming_ideas_that_arent_games_already/"
    },
    {
      "id": "1ixb3bn",
      "title": "How is a TM similar to a DFA or PDA?",
      "content": "I feel like the course on ToC wants to build up/motivate the concept of a Turing Machine. Going from a DFA to PDA was very natural, whereas from PDA to TM not so much. TM seems to be something completely different. Can you motivate a Turing Machine by talking about a PDA?",
      "author": "Valuable-Glass1106",
      "created_utc": 1740426574,
      "upvotes": 20,
      "upvote_ratio": 0.88,
      "num_comments": 22,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ixb3bn/how_is_a_tm_similar_to_a_dfa_or_pda/"
    },
    {
      "id": "1iwgu6o",
      "title": "I designed my own ternary computer",
      "content": "So I pretty much realised I will never have enough money to build this, and no school or university will accept my proposal (I'm in 11th grade and yes, I tried.) So I will just share it for free in the hopes of someone having the resources to build it. I tried to make the divider circuit too, but tbh, I just lost the willpower to do it since the realization. So here are the plans. Some of it is in Hungarian, but if you understand basic MOSFET logic, you will figure it out. I tried to make it similar to binary logic. From now on, I might just stop with designing this. The pictures include an adder, multiplier, some comparator circuits, and a half-finished divider. The other things (like memory handling, etc) are pretty easy to implement. It is just addressing. I have some other projects, like simulating a mach 17 plane and designing it, but eh, this is probably the \"biggest\" one. Oh and also, it is based on balanced ternary voltage (-1 volt is 2 0 = 0 1 volt is 1).\n\nProof that it works better:  \nMy multiplier (3x2)'s maximum output is 21201 (208) With \\~110 MOSFET-s. A 3x2 Binary multiplier takes 10-20 MOSFETs less, i think, but its maximum output is only a weak 21. And if we make a bigger multiplier, the bigger will be the difference. My design is more data-MOSFET compact than a binary one, which could make phones and servers more efficient (the two things that need to be.) And we could use the minus part of the Wi-Fi signal wave too! The possibilities are endless!\n\n[ternary \\\\\"or\\\\\"](https://preview.redd.it/kn68dyr4lxke1.jpg?width=4096&format=pjpg&auto=webp&s=23f0398d258c3b91e70cf0bce42cdfc7b8f32548)\n\n[Ternary \\\\\"and\\\\\"](https://preview.redd.it/ws1843s4lxke1.jpg?width=2304&format=pjpg&auto=webp&s=2bd7622db1f1a916a15c588d8db462bdbd4a3e07)\n\n[Comparator circuit \\(A\\>=B\\)](https://preview.redd.it/qfnhe0s4lxke1.jpg?width=4096&format=pjpg&auto=webp&s=69a87db12baa64cb57993ba13b24ee9d682c0198)\n\n[One trit divider](https://preview.redd.it/23z16dt4lxke1.jpg?width=2304&format=pjpg&auto=webp&s=6fd457b376b96ec1c9afc58720dd8e6ea8689156)\n\n[Basic logic circuits](https://preview.redd.it/hpdk61s4lxke1.jpg?width=2304&format=pjpg&auto=webp&s=2cd078d582f4e8bcec180eabd768f832d610fcf3)\n\n[Multiplier](https://preview.redd.it/9ougj5s4lxke1.jpg?width=2304&format=pjpg&auto=webp&s=8ab51f5f5f3771b984f27e092b65d447cc794657)",
      "author": "Xulum12",
      "created_utc": 1740335394,
      "upvotes": 165,
      "upvote_ratio": 0.98,
      "num_comments": 43,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iwgu6o/i_designed_my_own_ternary_computer/"
    },
    {
      "id": "1iwg945",
      "title": "computers in minecraft",
      "content": "I'm sure you've all seen those awesome redstone computers in Minecraft before, but it got me thinking - the limitations of our computers are resources, and space, neither of which are limitations in Minecraft creative mode. I know the computers previously built in Minecraft are no-where near even the capability of a phone yet, but hypothetically, could a computer in Minecraft be more powerful than the very one it is built through? (whether or not its capability could be done justice) if so, how much more powerful?",
      "author": "o-artemis-o",
      "created_utc": 1740333924,
      "upvotes": 87,
      "upvote_ratio": 0.79,
      "num_comments": 40,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iwg945/computers_in_minecraft/"
    },
    {
      "id": "1iwf8qi",
      "title": "What does it actually mean for us, when a DFA accepts a string?",
      "content": "I feel like I've gone fairly far, without asking the obvious. Why do we care that an automaton accepts some input? I get it that it's supposed to be a computing model, but don't computers spit out something meaningful? Where here as output we get accept, reject or halt (for TM).\n\nEdit: Lots of interesting and insightful answers. God, I love this sub! I'm self studying this subject and the fact that so many people are willing to talk to me (even though they don't even know me and I will never pay them back) are spending their time to answer my question is what makes science (and life) beautiful! Big thank you to all!",
      "author": "Valuable-Glass1106",
      "created_utc": 1740331355,
      "upvotes": 36,
      "upvote_ratio": 0.92,
      "num_comments": 22,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iwf8qi/what_does_it_actually_mean_for_us_when_a_dfa/"
    },
    {
      "id": "1ivoj05",
      "title": "Understanding the social aspects and stereotypes of CS majors",
      "content": "I am a current CS student and when meeting other non-CS students I immediately get that \"oh, cool...\" and that's it. I am aware of the base stereotype that they tend to be \"quirky\" but I am really curious if anyone has any deep insight on why others have this immediate outlook.",
      "author": "GetShlumpd",
      "created_utc": 1740246610,
      "upvotes": 8,
      "upvote_ratio": 0.68,
      "num_comments": 14,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ivoj05/understanding_the_social_aspects_and_stereotypes/"
    },
    {
      "id": "1iv1mz3",
      "title": "How do you guys read these books?",
      "content": "Hey everyone,\n\nI just bought my first two computer science books: Clean Architecture by Uncle Bob and Designing Data-Intensive Applications by Martin Kleppmann. This is a bit of a shift for me because I've always been someone who learned primarily through videos—tutorials, lectures, and hands-on coding. But lately, I’ve realized that books might offer a deeper, more structured way to learn, and a lot of people have recommended these titles.\n\nThat said, I’m a bit unsure about how to approach reading them. Do you just read through these kinds of books like a story, absorbing the concepts as you go? Or do you treat them more like textbooks—taking intensive notes, breaking down diagrams, and applying what you learn through practice?\n\nI’d love to hear how you tackle these books specifically or any CS books in general. How do you make sure you’re really retaining and applying the knowledge?\n\nAppreciate any advice!\n\n",
      "author": "Sandwizard16",
      "created_utc": 1740171798,
      "upvotes": 266,
      "upvote_ratio": 0.97,
      "num_comments": 85,
      "flair": "Advice",
      "url": "https://i.redd.it/ozddj3t64kke1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1iv1mz3/how_do_you_guys_read_these_books/"
    },
    {
      "id": "1ivnmml",
      "title": "feedback loop in Charles Petzold book \"Code\"",
      "content": "https://preview.redd.it/0g7r5deg3qke1.png?width=1262&format=png&auto=webp&s=f83a2f513c2e7c54454dee0b4d3405c5282779f0\n\nIn this part it says that only current flowing in this circuit is from the output of the left NOR gate and that's because both inputs to that gate are 0. I don't understand how are both inputs to the left gate 0 if the two NOR logic gates are both dependent to each other. Is it just randomly assigned to have starting point or is there some logic? I'm confused ",
      "author": "wuweei",
      "created_utc": 1740244383,
      "upvotes": 5,
      "upvote_ratio": 1.0,
      "num_comments": 7,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ivnmml/feedback_loop_in_charles_petzold_book_code/"
    },
    {
      "id": "1iup05f",
      "title": "What do Hardware Optimisation and Software Optimisation mean? Particularly for phones.",
      "content": "Not sure if this is the right sub. If not, ***please*** direct me to the right one.\n\nRegardless, *any* pointers in the right direction would be much appreciated, of course if you're able :)",
      "author": "themaskstays_",
      "created_utc": 1740138343,
      "upvotes": 9,
      "upvote_ratio": 0.85,
      "num_comments": 11,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iup05f/what_do_hardware_optimisation_and_software/"
    },
    {
      "id": "1iu50p4",
      "title": "Do you feel the future of computers performance will be found in writing in assembly?",
      "content": "I’m surprised we haven’t been using all the new tools we have today to reverse engineer assembly languages. Would we get any significant boost in performance by looking at lower levels of code or would that just muddle it?",
      "author": "haitianCook",
      "created_utc": 1740074914,
      "upvotes": 34,
      "upvote_ratio": 0.67,
      "num_comments": 61,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iu50p4/do_you_feel_the_future_of_computers_performance/"
    },
    {
      "id": "1itqnyv",
      "title": "Which book is good for Computer Architetcure",
      "content": "Computer Systems A Programmer's Perspective Bryant O'Hallaron or Computer organization and design Patterson Hennsy\n\nIm following teachyourselfcs \\\\.com and they recommend these two books\n\nI've already done the first 6 chapters of nand2tetris so my question is which one of these should i choose. I was following along a programmers prespective but it gets confusing around chapter three (mostly having to learn a bit of assembly)\n\nshould i continue with BryantOhallaron after learning assembly or PattersonHensy?",
      "author": "AdRoyal3912",
      "created_utc": 1740027704,
      "upvotes": 37,
      "upvote_ratio": 0.97,
      "num_comments": 21,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1itqnyv/which_book_is_good_for_computer_architetcure/"
    },
    {
      "id": "1itmxoy",
      "title": "How Computers Actually Work?",
      "content": "Hi I am working on a [blog](https://medium.com/@ishantaldekar1/how-computers-actually-work-a-risc-v-perspective-4a493a64c40f) that goes over the fundamentals of Computer System Architecture in brief. I have really bad memory, so I wanted something short that I could use to refresh the concepts when I need to. I wanted to share it with you guys, if you're interested! Please let me know if I can improve anything, or if I get something wrong!",
      "author": "who_is_me_here",
      "created_utc": 1740015913,
      "upvotes": 41,
      "upvote_ratio": 0.75,
      "num_comments": 33,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1itmxoy/how_computers_actually_work/"
    },
    {
      "id": "1itbn7e",
      "title": "HashTables and runtimes",
      "content": "Here’s the optimal solution for the Two Sum problem on LeetCode. The solution uses a hash map (defined with “Dictionary” in C#). I understand that this solution improves upon the brute force solution in terms of time complexity with a runtime of O(n) over O(n*2)\n\nI’m wondering as to how hash map accessing works however? How do these lookups have a complexity of O(1) instead of O(n) exactly? Do you not need to iterate through the hash map itself? ",
      "author": "nineinterpretations",
      "created_utc": 1739987318,
      "upvotes": 41,
      "upvote_ratio": 0.89,
      "num_comments": 14,
      "flair": "Help",
      "url": "https://i.redd.it/8ufqqoqmv4ke1.jpeg",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1itbn7e/hashtables_and_runtimes/"
    },
    {
      "id": "1isv352",
      "title": "JesseSort is getting faster...",
      "content": "Pushed a C++ version and tested it against std::sort on 2\\^24 values.\n\n    JesseSort: 2.24347 seconds\n    std::sort: 0.901765 seconds\n\nGetting closer... Still haven't implemented Powersort's optimal merge tree and this version is missing the saved index locations between loops. Anyway, I'm excited so I thought I'd share. Have a good night!\n\nEdit: Just realized this is also missing the base array copies. I bet that'll speed it up too!",
      "author": "booker388",
      "created_utc": 1739933529,
      "upvotes": 161,
      "upvote_ratio": 0.91,
      "num_comments": 22,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1isv352/jessesort_is_getting_faster/"
    },
    {
      "id": "1iscvpp",
      "title": "About deleted files",
      "content": "When we delete a file system make there unallocated and just delete the pointers. But why does system also delete the file itself. I mean if data and pointer next to each other it can be a fast operatin, at least for some types of documents. What am I missing an not knowing here.\nAnd how the hard drive know it's own situation about the emptiness and fullness? Does hard drive has a special space for this? ",
      "author": "nonMaterialAlchemist",
      "created_utc": 1739885984,
      "upvotes": 7,
      "upvote_ratio": 0.58,
      "num_comments": 20,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iscvpp/about_deleted_files/"
    },
    {
      "id": "1irmz7k",
      "title": "Where can my son get feedback on his coding projects?",
      "content": "Hi my son is 12 and is miles ahead of the work that he is being taught at school for computer science (UK). \n\nHe completed CS50 last year and really enjoyed it.\n\nHe's currently 3/4 of the way through making his own game engine and I'd like find someone that he could talk to about his current projects and get some advice or feedback.\n\nDoes anyone have any recommendations? Maybe a tutor or is there a discord server that he could join or something like that (I'm a bit hesitant to let him on discord because I don't want him getting groomed).\n\nI feel bad that he's so passionate about coding and has no one to talk to about it that understands what he's talking about.",
      "author": "Minute_Ad_3719",
      "created_utc": 1739807601,
      "upvotes": 101,
      "upvote_ratio": 0.81,
      "num_comments": 36,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1irmz7k/where_can_my_son_get_feedback_on_his_coding/"
    },
    {
      "id": "1iqvj4n",
      "title": "Updates on JesseSort",
      "content": "tl;dr I came up with a new sorting algorithm based on a new data structure. Original post was here: [https://www.reddit.com/r/computerscience/comments/1ion02s/a\\_new\\_sorting\\_algorithm\\_for\\_2025\\_faster\\_than/](https://www.reddit.com/r/computerscience/comments/1ion02s/a_new_sorting_algorithm_for_2025_faster_than/)\n\nSince that post 3 days ago, I've gotten tons of feedback and support from folks. Made contact with Sebastian Wild (Powersort) about possible collaboration. Still have to run stability analysis and memory analysis and test it on various types of inputs and add a formal proof... Lots to do!\n\nOne person identified JesseSort's insertion logic as a variation on Patience Sort, so I read up on it. I had never heard of Patience Sort, and it seems to be a sorting algorithm that's generally flown under the radar. Possibly dismissed because it has an extremely common worst case: if your stacks (what I call \"bands\") are descending and your unsorted input is a natural ascending run, or if your stacks are ascending and your unsorted input is a natural descending run, then you're going to make n stacks and it becomes plain old mergesort with extra wasted time/space to run the useless insertion phase. As natural runs are so common in real data, running into one of these worst cases makes the algorithm a terrible choice like 50% of the time lol.\n\nInterestingly enough, I came up with the solution to this problem without even knowing about it! Split Rainbows divide the inputs to essentially play 2 games of Patience: one with descending stacks (lower half of the Rainbow) and one with ascending stacks (upper half of the Rainbow). The difference is that my current implementation makes the bottom half values go from roughly \\[1, n/2\\] and top half from \\[n/2, n\\]. Patience just uses a \"Half Rainbow\" traditionally, but goes through all \\[1, n\\] values. Now that I know more, I may tweak the code to formally split these Rainbow halves into separate structures and use 2 separate base arrays to play these 2 games of Patience with full ranges from \\[1, n\\]. Something like this:\n\n    # Process remaining values\n    for i in range(4, n):\n        # Check for ascending vs descending natural runs to send this new value to the best game\n        if unsorted_array[i] > last_value_processed: # We're in an ascending natural run\n            which_half_rainbow = half_rainbow_ascending_bands\n            which_band_sizes = band_sizes_ascending_bands\n            which_band_capacities = band_capacities_ascending_bands\n            which_base_array = base_array_ascending_bands\n            ... etc\n        elif unsorted_array[i] < last_value_processed: # We're in a descending natural run\n            which_half_rainbow = half_rainbow_descending_bands\n            ... etc\n        # else do nothing to use the same half rainbow as last loop to process repeated value in O(n)\n        jessesort_with_half_rainbows_and_value_array(\n            &which_half_rainbow, &which_band_sizes, &which_band_capacities, &which_base_array, &which_arr_size, &which_arr_capacity, &which_mid, &(unsorted_array[i])\n            )\n        last_value_processed = unsorted_array[i]\n\nThis sends the new value to the better game of Patience with its own half rainbow and base array. Powersort's optimal merge tree is still planned for the merging phase. Obviously more testing is needed as you're watching JesseSort's development unfold live, but wanted to share what's happening. I find all of this exciting!\n\nI've mentioned this 100x already but sorting isn't my area of expertise yet, so I still have a lot to learn and implement with JesseSort. Thank you guys for being so supportive and giving me great feedback, ideas, and knowledge gaps to read up on.",
      "author": "booker388",
      "created_utc": 1739722259,
      "upvotes": 193,
      "upvote_ratio": 0.95,
      "num_comments": 9,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iqvj4n/updates_on_jessesort/"
    },
    {
      "id": "1iqrg1u",
      "title": "1bit half adder in dominoes",
      "content": "Made a 1bit half adder in dominoes. Left gate is a XOR gate between blue and orange for the sum and right gate is a an AND gate for carrying bit output.",
      "author": "FloBEAUG",
      "created_utc": 1739709871,
      "upvotes": 230,
      "upvote_ratio": 0.98,
      "num_comments": 14,
      "flair": null,
      "url": "https://i.redd.it/kka38k2nyhje1.png",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1iqrg1u/1bit_half_adder_in_dominoes/"
    },
    {
      "id": "1iqlbex",
      "title": "Proofs",
      "content": "Proofs in computer science math confuses me and I think it would help to have some good examples for each to reference so if you have the time to offer a simple example of one of these proofs that would be greatly appreciated, I keep getting some questions wrong on my tests and I don't know why.\n\n1. Direct: Most simple statements can be proved directly. No keyword really “gives away” the impression that this method of proof is needed.\n2. Contrapositive: If-then statements where Q has phrases like ‘for all’ or ‘for every’ can sometimes be more easily proven by writing and proving the contrapositive of the whole statement.\n3. Contradiction: If-then statements where you suspect “P and not Q” is false can be best proven by contradiction.\n4. Induction: Almost any statement with summations or recursions is best proved by induction or strong induction. The “Induction and Strong Induction” lesson will dive deeper into this technique.\n5. Exhaustion: Any statement that suggests the existence of some property for every number can be proven by showing directly that every number has that property.\n6. Existence: Any statement asserting the existence of a number with a given property can be proven using this method.\n7. Proof by Counterexample: Any statement that suggests every number has a certain property can be disproven if you can provide a number that does not have that property.",
      "author": "Sider4life",
      "created_utc": 1739684164,
      "upvotes": 30,
      "upvote_ratio": 0.97,
      "num_comments": 5,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iqlbex/proofs/"
    },
    {
      "id": "1ipzggn",
      "title": "Why is CS one subject of study?",
      "content": "Computer networks, databases, software engineering patterns, computer graphics, OS development\n\nI get that the theoretical part is studied (formal systems, graph theory, complexity theory, decidability theory, descrete maths, numerical maths) as they can be applied almost everywhere. \n\nBut like wtf? All these applied fields have really not much in common. They all use theoretical CS in some extends but other than that? Nothing. \n\nThe Bachelor feels like running through all these applied CS fields without really understanding any of them. \n\n\nEDIT It would be similar to studying math would include every field where math is applied ",
      "author": "largetomato123",
      "created_utc": 1739618967,
      "upvotes": 204,
      "upvote_ratio": 0.85,
      "num_comments": 74,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ipzggn/why_is_cs_one_subject_of_study/"
    },
    {
      "id": "1iq9kh5",
      "title": "Variations of Von Neumann Architecture",
      "content": "Help: my professor asked us to research on variations of Von Neumann Architecture. My classmates keep submitting answers differentiating Von Neumann and Harvard Architecture but I find it to be completely different from Von Neumann - meaning that it's a complete departure and not just a variation. To give more context, the question is : What are the different variations of Von Neumann model and compare it to the original version. I have been researching but I seem to not get variations but just comparison to Harvard Architecture so it makes me think if I'm just overthinking the question. Is there really such thing as variations of Von Neumann? Thanks! \n\nEdit: Thanks everyone! Your inputs were all helpful! ",
      "author": "fudgy-cake",
      "created_utc": 1739648823,
      "upvotes": 17,
      "upvote_ratio": 1.0,
      "num_comments": 10,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iq9kh5/variations_of_von_neumann_architecture/"
    },
    {
      "id": "1iq6768",
      "title": "Convirgance - Alternative to ORMs (AMA)",
      "content": "[Web Service in 5 Iines of code](https://preview.redd.it/pd9s0hbp6cje1.png?width=2583&format=png&auto=webp&s=53084f3b02eefe881e01d8d9d5400e8b8d317ac7)\n\nI recently saw a post by a redditor who said they miss using CompSci theory and practice in the industry. That their work is repetitive and not fulfilling.\n\nThis one hits me personally as I've been long frustrated by our industry's inability to advance due to a lack of commitment to software engineering as a discipline. In a mad race to add semi-skilled labor to the market, we’ve ignored opportunities to use software engineering to deliver orders of magnitude faster.\n\nI’m posting this AMA so we can talk about it and see if we can change things.\n\n*Who are you?*\n\nMy name is Jerason Banes. I am a software engineer and architect who has been lucky enough to deliver some amazing solutions to the market, but have also been stifled by many of the challenges in today’s corporate development.\n\nI’ve wanted to bring my learnings on Software Engineering and Management to the wider CompSci community for years. However, the gulf of describing solutions versus putting them in people’s hands is large. Especially when they displace popular solutions. Thus I quit my job back in September and started a company that is producing MIT-licensed Open Source to try and change our industry.\n\n*What is wrong with ORMs?*\n\nI was part of the community that developed ORMs back around the turn of the century. What we were trying to accomplish and what we got were two different things entirely. That’s partly because we made a number of mistakes in our thinking that I’m happy to answer questions about.\n\nSuffice it to say, ORMs drive us to design and write sub-standard software that is forced to align to an object model rather than aligning to scalable data processing standards.\n\nFor example, I have a pre-release OLAP engine that generates SQL reports. It can’t be run on an ORM because there’s no stable list of columns to map to. Similarly, the queries we feed into “sql mapper” type of ORMs like JOOQ just can’t handle complex queries coming from the database without massively blowing out the object model.\n\nAt one point in my career I noticed that 60% of code written by my team was for ORM! Ditching ORMs saved all of that time and energy while making our software BETTER and more capable.\n\nI am far from the only one sounding the alarm on this. The well known architect Ted Neward wrote \"The Vietnam of Computer Science\" back in 2006. And Laurie Voss of NPM fame called ORMs an \"anti-pattern\" back in 2011.\n\nBut what is the alternative?\n\n*What is Convirgance?*\n\nConvirgance aims to solve the problem of data handling altogether. Rather than attempting to map everything to carrier objects (DTOs or POJOs), it puts each record into a Java Map object, allowing arbitrary data mapping of any SQL query.\n\nThe Java Map (and related List object) are presented in the form of \"JSON\" objects. This is done to make debugging and data movement extremely easy. Need to debug a complex data record? Just print it out. You can even pretty print it to make it easier to read.\n\nConvirgance scales through its approach to handling data. Rather than loading it all into memory, data is streamed using Iterable/Iterator. This means that records are handled one at a time, minimizing memory usage.\n\nThe use of Java streams means that we can attach common transformations like filtering, data type transformations, or my favorite: pivoting a one-to-many join into a JSON hierarchy. e.g.\n\n    {\"order_id\": 1, \"products\": 2, \"line_id\": 1, \"product\": \"Bunny\", \"price\": 22.95}\n    {\"order_id\": 1, \"products\": 2, \"line_id\": 2, \"product\": \"AA Batteries\", \"price\": 8.32}\n\n…becomes:\n\n    {\"order_id\": 1, \"products\": 2, lines: [\n      {\"line_id\": 1, \"product\": \"Bunny\", \"price\": 22.95},\n      {\"line_id\": 2, \"product\": \"AA Batteries\", \"price\": 8.32}\n    ]}\n\nFinally, you can convert the data streams to nearly any format you need. We supply JSON (of course), CSV, pipe & tab delimited, and even a binary format out of the box. We’re adding more formats as we go.\n\nThis simple design is how we’re able to create slim web services like the one in the image above. Not only is it stupidly simple to create services, we’ve designed it to be configuration driven. Which means you could easily make your web services even smaller. Let me know in your questions if that’s something you want to talk about!\n\nDocumentation: [https://convirgance.invirgance.com](https://convirgance.invirgance.com)\n\nThe code is available on GitHub if you want to read it. Just click the link in the upper-right corner. It’s quite simple and straightforward. I encourage anything who’s interested to take a look.\n\n*How does this relate to CompSci?*\n\nConvirgance seems simple. And it is. In large part because it achieves its simplicity through machine sympathy. i.e. It is designed around the way computers work as a machine rather than trying to create an arbitrary abstraction.\n\nThis machine sympathy allowed us to bake a lot of advantages into the software:\n\n* Maximum use of the Young Generation garbage collector. Since objects are streamed through one at a time and then released, we’re unlikely to overflow into \"old\" space. The Young collector is known to have performance that sometimes exceeds C malloc!\n* Orders of magnitude more CPU cycles available due to better L1 and L2 caching. Most systems (including ORMs) perform transformations on the entire in-memory set. One at a time. This is unkind to the CPU cache, forcing repetitive streaming to and from main memory with almost no cache utilization. The Convirgance approach does this stream from memory only once, performing all scheduled computation on each object before moving on to the next.\n* Lower latency. The decision to stream one object at a time means that the data is being processed and delivered before all data is available. This balances the use of I/O and CPU, making sure all components of the computer are engaged simultaneously.\n* Faster query plans. We’ve been told to bind our variables for safety without being told the cost to the database query planner. The planner needs the values to effectively partition prune, select the right indexes, choose the right join algorithm, etc. Binding withholds those values until after the query planner is chosen. Convirgance changes this by performing safe injection of bind variables to give the database what it needs to perform.\n\nThese are some of the advantages that are baked into the approach. However, we’ve still left a lot of performance on the table for future releases. Feel free to ask if you want to understand any of these attributes better or want to know more about what we’re leaving on the table.\n\n*What types of questions can I ask?*\n\nAnything you want, really. I love Computer Science and it’s so rare that I get to talk about it in depth. But to help you out, here are some potential suggestions:\n\n* General CompSci questions you’ve always wanted to ask\n* The Computer Science of Management\n* Why is software development so slow and how can CompSci help?\n* Anything about Convirgance\n* Anything about my company Invirgance\n* Anything you want to know about me. e.g. The popular DSiCade gaming site was a sneaky way of testing horizontal architectures back around 2010.\n* Why our approach of using semi-skilled labor over trained CompSci labor isn’t working\n* Will LLMs replace computer scientists? (No.) How does Convirgance fit into this?\n* You mentioned building many technologies. What else is coming and why should I care as a Computer Scientist?",
      "author": "thewiirocks",
      "created_utc": 1739639964,
      "upvotes": 15,
      "upvote_ratio": 0.89,
      "num_comments": 15,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iq6768/convirgance_alternative_to_orms_ama/"
    },
    {
      "id": "1ion02s",
      "title": "A new sorting algorithm for 2025, faster than Powersort!",
      "content": "tl;dr It's faster than Python's Default sorted() function, Powersort, and it's not even optimized yet. See chart below:\n\nhttps://preview.redd.it/z75sze5klxie1.png?width=566&format=png&auto=webp&s=ef7d4c2ff96382066b5e51c67dbdbe4436cfb709\n\nJesseSort uses a Rainbow data structure to keep search space small. A Rainbow is an array of array-like structures with a special feature: the first and last values of each subsequent subarray are guaranteed to be in sorted order. The \"base array\" in the black rectangle represents the search space for inserting new values. As a band can have near-infinite values in it and its search space remains its 2 ends, one can easily see how JesseSort offers value at scale.\n\nhttps://preview.redd.it/xubl2jjvlxie1.png?width=484&format=png&auto=webp&s=1614ba859515a44e61f9f60ce4f26662a5dc4d52\n\n[Base array in the black rectangle](https://preview.redd.it/aenpy4w3mxie1.png?width=484&format=png&auto=webp&s=9197e9f7cb72c31a9fe16cb59d4e60fe97f56487)\n\nJesseSort has 2 main phases: insertion and merging. For each new value, do binary search for insertion index inside base array in log time, insert value into band (front or back of it), replace value in base array, loop. Then merge all bands until one remains. To avoid front-end insertion issues, we split the Rainbow bands into 2 halves and reverse the order of the bottom half subarrays.\n\nCode and paper here: [https://www.github.com/lewj85/jessesort](https://www.github.com/lewj85/jessesort)\n\nEdit: This is apparently related to Patience Sort. And I've unexpectedly solved its biggest issue by essentially playing 2 games of Patience simultaneously, one with descending stacks and one with ascending stacks. I provide an update here: [https://www.reddit.com/r/computerscience/comments/1iqvj4n/updates\\_on\\_jessesort/](https://www.reddit.com/r/computerscience/comments/1iqvj4n/updates_on_jessesort/)",
      "author": "booker388",
      "created_utc": 1739464176,
      "upvotes": 895,
      "upvote_ratio": 0.97,
      "num_comments": 84,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ion02s/a_new_sorting_algorithm_for_2025_faster_than/"
    },
    {
      "id": "1ip1w63",
      "title": "Getting into cs research",
      "content": "I was wondering what are the different domains in cs research? How does one get into this field? I'm a freshman in uni doing cs rn and i want to try this out as well.\n\n I understand cs research is actually the study of computation which is essentially math, but I'm unable to find further on this topic in a language i understand. This is coming from someone who doesn't know how to use Google scholar or read a paper.b can someone explain it to me in simple terms and maybe suggest some resources? I'd be very grateful:D\n\nSorry if this is too stupid of a question for this sub\n",
      "author": "EmotionallyPoor",
      "created_utc": 1739505213,
      "upvotes": 35,
      "upvote_ratio": 1.0,
      "num_comments": 13,
      "flair": "Advice",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ip1w63/getting_into_cs_research/"
    },
    {
      "id": "1io6mqp",
      "title": "I miss doing real computer science",
      "content": "I saw something that said “in industry basically 95% of what you do is just fancy CRUD operations”, and came to realize that held true for basically anything I’ve done in industry. It’s boring \n\nI miss learning real computer science in school. Programming felt challenging, and rewarding when it was based in theory and math. \n\nIn most industry experience we use frameworks which abstract away a lot, and everything I’ve worked on can be (overly) simplified down to a user frontend that asks a backend for data from a database and displays it. It’s not like the apps aren’t useful, but they are nothing new, nothing that hasn’t been done before, and don’t require any complex thinking, science, or math in many ways. ",
      "author": "Nameless0616",
      "created_utc": 1739407229,
      "upvotes": 1942,
      "upvote_ratio": 0.98,
      "num_comments": 194,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1io6mqp/i_miss_doing_real_computer_science/"
    },
    {
      "id": "1ip3mw0",
      "title": "If software is just 1s and 0s, why can't we just manually edit a program's binary to fix bugs? Wouldn't that be easier than waiting for patches? (I’m new to this)",
      "content": "I know this sounds dumb, but hear me out. If all software is just binary (1s and 0s), then in theory, shouldn’t we be able to open up an executable file, find the part that's broken, and just... change the bits? Like if a game is crashing, why not just flip some 0s to 1s and fix it ourselves instead of waiting for devs to drop a patch? What actually makes this impossible? Genuinely curious.",
      "author": "Fluid_Discipline7284",
      "created_utc": 1739511340,
      "upvotes": 5,
      "upvote_ratio": 0.55,
      "num_comments": 50,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ip3mw0/if_software_is_just_1s_and_0s_why_cant_we_just/"
    },
    {
      "id": "1iojekx",
      "title": "How can I turn my brain into an engineer's brain?",
      "content": "In courses such as Digital Design, Algorithms, Discrete Math etc. I sometimes have difficulty in finding solutions. When I find solutions, I usually take a difficult path (I have difficulty in discovering optimized paths). I want to improve myself in this respect. I want to be more practical, agile, maybe smarter. I will graduate in 2 years. I want to put things in order already, what can I do?",
      "author": "Ced3j",
      "created_utc": 1739454486,
      "upvotes": 88,
      "upvote_ratio": 0.9,
      "num_comments": 55,
      "flair": "General",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1iojekx/how_can_i_turn_my_brain_into_an_engineers_brain/"
    },
    {
      "id": "1ip1c8v",
      "title": "(Please be kind) I need to find a way to appreciate computer science.",
      "content": "I hope I can ask this here because I’m a little desperate. I want to learn to love computers and how they work.\n\nI feel nothing when it comes to them, but I want to understand their science. I’m a natural science person at best and just have never cared for them, even with a little disdain.\n\nWhere did your love start? Who was your Steve Irwin or Bill Nye? Something? A YouTube video or book?",
      "author": "AaaIdkWhat",
      "created_utc": 1739503385,
      "upvotes": 10,
      "upvote_ratio": 0.57,
      "num_comments": 57,
      "flair": "Help",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ip1c8v/please_be_kind_i_need_to_find_a_way_to_appreciate/"
    },
    {
      "id": "1ion2gm",
      "title": "What RFCs are your favourite, based on enjoyable reading?",
      "content": "I recently read rfc 7636. It's extremely short and concise, I thought \"wow, this is the some of the best documentation on anything I've ever read!\" What are some other rfcs that are well written and very enjoyable to read?",
      "author": "jeesuscheesus",
      "created_utc": 1739464338,
      "upvotes": 14,
      "upvote_ratio": 0.95,
      "num_comments": 4,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1ion2gm/what_rfcs_are_your_favourite_based_on_enjoyable/"
    },
    {
      "id": "1inwzoo",
      "title": "What is the point of computational models?",
      "content": "I'm in a computational models class right now, and I'm frankly sick of drawing endless diagrams of DFAS that involve drawing ten thousand circles, and proving if some random string of numbers would be a regular language. I also kind of don't see how I would ever possibly use the information I've learned in this class. \n\n  \nBut, at the same, I didn't really see why Vector Calculus was a required class for CS majors until I got more into ML stuff, and now I totally get it, so maybe if I'm just missing some context, so I wanted to ask to possibly get the opinion of someone further on in their CS journey.\n\n  \nStudying for something sucks less when you know why you're doing it, so I'm curious about what the point of studying computational models is and why it might be a required class.",
      "author": "por_eso_xpresso",
      "created_utc": 1739382762,
      "upvotes": 31,
      "upvote_ratio": 0.8,
      "num_comments": 17,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1inwzoo/what_is_the_point_of_computational_models/"
    },
    {
      "id": "1indsbv",
      "title": "Meta languages, and declaring an object language",
      "content": "I was recently studying a bit of (programming) language theory. You know the basics; setting up a language based on a set (of words) with some terminal/non-terminal grammar, such as with BNF, etc. to create functionality. You create a new language by describing it with a meta language. And by describing said new language, you have created an object language. So my question is, when does this overlap happen?\n\nIf I were to describe English with a finite set of words, and so-and-so rules using mathematics, is English therefore an object language? And the other way around; if I were to describe a derivative language, say from C++, which is essentially a derivative of a variety of languages, thus technically an object language, is C++ then also a meta language?\n\nIs meta/object language just a label? Because my understanding is that as soon as you use language \"A\" to describe a new- \"B\", then \"A\" is the meta language, and \"B\" is therefore the object language.",
      "author": "Dr_Dressing",
      "created_utc": 1739319272,
      "upvotes": 7,
      "upvote_ratio": 0.89,
      "num_comments": 4,
      "flair": "Discussion",
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1indsbv/meta_languages_and_declaring_an_object_language/"
    },
    {
      "id": "1in9hjv",
      "title": "NearestCity search",
      "content": "So today I had to go into an office for a interview and I had to do a pen and paper test.\nI had had 55 minutes to answer 6 questions. (9 mins per question)\n4 questions were bug fixes/ removing redundant code in short chunks of code.\n5th question was creating a stack class with pop and push without using any collections APIs.\n(I didn’t know what stack was at this point in time, I’d have never used it in my day to day work)\n\n6th question was : you have 1,000,000 cities and their x and y coordinates. How would you go about finding the nearest city given a random x,y coordinates. Find the best and fastest solution.\n(Pen and paper test, no internet access)\n\nHonestly the last question was a bit of a misleading question because the snr dev came to discuss the answers and was guiding me towards a very specific algorithm and data structure I wasn’t aware/ could remember.\n\n\nHow would you go about the last question?",
      "author": "raul36412",
      "created_utc": 1739308141,
      "upvotes": 11,
      "upvote_ratio": 0.92,
      "num_comments": 5,
      "flair": null,
      "url": null,
      "is_self": true,
      "permalink": "/r/computerscience/comments/1in9hjv/nearestcity_search/"
    },
    {
      "id": "1imdtxj",
      "title": "Undergraduate Upends a 40-Year-Old Data Science Conjecture | Quanta Magazine",
      "content": null,
      "author": "RstarPhoneix",
      "created_utc": 1739213764,
      "upvotes": 323,
      "upvote_ratio": 0.99,
      "num_comments": 17,
      "flair": null,
      "url": "https://www.quantamagazine.org/undergraduate-upends-a-40-year-old-data-science-conjecture-20250210/",
      "is_self": false,
      "permalink": "/r/computerscience/comments/1imdtxj/undergraduate_upends_a_40yearold_data_science/"
    }
  ]
}